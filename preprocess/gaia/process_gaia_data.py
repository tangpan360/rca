import os
import sys
import pickle
import numpy as np
import pandas as pd
from tqdm import tqdm
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥utils
_script_dir = os.path.dirname(os.path.abspath(__file__))
_project_root = os.path.dirname(os.path.dirname(_script_dir))
sys.path.append(_project_root)

from utils.template_utils import get_log_template_count

# å…¨å±€å®šä¹‰ï¼šæ‰€æœ‰instanceçš„å›ºå®šé¡ºåºï¼ˆç”¨äºæ‰€æœ‰æ¨¡æ€ï¼‰
SERVICES = ['dbservice1', 'dbservice2', 'logservice1', 'logservice2', 
            'mobservice1', 'mobservice2', 'redisservice1', 'redisservice2', 
            'webservice1', 'webservice2']

# å…¨å±€ç¼“å­˜ï¼šé¢„åŠ è½½çš„æ•°æ®
METRIC_DATA_CACHE = {}
LOG_DATA_CACHE = {}
TRACE_DATA_CACHE = {}

# å…¨å±€å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»è®­ç»ƒé›†è®¡ç®—ï¼‰
NORMALIZATION_STATS = {
    'metric': None,  # {'mean': [12], 'std': [12]}
    'log': None,     # {'mean': [48], 'std': [48]}
    'trace': None    # [{'mean': float, 'std': float}] * 10 (for duration only)
}


def preload_all_data():
    """
    é¢„åŠ è½½æ‰€æœ‰æ¨¡æ€çš„æ•°æ®åˆ°å†…å­˜
    """
    print("=" * 50)
    print("å¼€å§‹é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜...")
    print("=" * 50)
    
    # 1. é¢„åŠ è½½ Metric æ•°æ®
    print("\n[1/3] åŠ è½½ Metric æ•°æ®...")
    metric_data_dir = os.path.join(_project_root, 'data', 'processed_data', 'gaia', 'metric')
    for instance_name in tqdm(SERVICES, desc="Metric"):
        metric_file = os.path.join(metric_data_dir, f"{instance_name}_metric.csv")
        if os.path.exists(metric_file):
            df = pd.read_csv(metric_file)
            METRIC_DATA_CACHE[instance_name] = df
            print(f"  âœ“ {instance_name}: {len(df)} è¡Œæ•°æ®")
    
    # 2. é¢„åŠ è½½ Log æ•°æ®
    print("\n[2/3] åŠ è½½ Log æ•°æ®...")
    log_data_dir = os.path.join(_project_root, 'data', 'processed_data', 'gaia', 'log')
    for instance_name in tqdm(SERVICES, desc="Log"):
        log_file = os.path.join(log_data_dir, f"{instance_name}_log.csv")
        if os.path.exists(log_file):
            # åªè¯»å–éœ€è¦çš„åˆ—ä»¥èŠ‚çœå†…å­˜
            df = pd.read_csv(log_file, usecols=['timestamp_ts', 'template_id'])
            LOG_DATA_CACHE[instance_name] = df
            print(f"  âœ“ {instance_name}: {len(df)} è¡Œæ•°æ®")
    
    # 3. é¢„åŠ è½½ Trace æ•°æ®
    print("\n[3/3] åŠ è½½ Trace æ•°æ® (åŒ…å«status_code)...")
    trace_data_dir = os.path.join(_project_root, 'data', 'processed_data', 'gaia', 'trace')
    for instance_name in tqdm(SERVICES, desc="Trace"):
        trace_file = os.path.join(trace_data_dir, f"{instance_name}_trace.csv")
        if os.path.exists(trace_file):
            # è¯»å– duration å’Œ status_code
            df = pd.read_csv(trace_file, usecols=['start_time_ts', 'duration', 'status_code'])
            TRACE_DATA_CACHE[instance_name] = df
            print(f"  âœ“ {instance_name}: {len(df)} è¡Œæ•°æ®")
    
    print("\n" + "=" * 50)
    print("æ•°æ®é¢„åŠ è½½å®Œæˆï¼")
    print(f"  Metric: {len(METRIC_DATA_CACHE)} ä¸ªå®ä¾‹")
    print(f"  Log: {len(LOG_DATA_CACHE)} ä¸ªå®ä¾‹")
    print(f"  Trace: {len(TRACE_DATA_CACHE)} ä¸ªå®ä¾‹")
    print("=" * 50 + "\n")


def load_anomaly_periods(label_file_path):
    """
    åŠ è½½å¼‚å¸¸æ—¶é—´æ®µæ•°æ®ï¼ˆå›ºå®š600ç§’çª—å£ï¼‰
    
    Args:
        label_file_path (str): æ ‡ç­¾æ–‡ä»¶è·¯å¾„
        
    Returns:
        list: å¼‚å¸¸æ—¶é—´æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(start_timestamp, end_timestamp, data_type)çš„ä¸‰å…ƒç»„
    """
    print("æ­£åœ¨åŠ è½½å¼‚å¸¸æ—¶é—´æ®µæ•°æ®...")
    
    # è¯»å–æ ‡ç­¾æ–‡ä»¶
    label_df = pd.read_csv(label_file_path)
    
    # è½¬æ¢æ—¶é—´æ ¼å¼ä¸ºæ—¶é—´æˆ³ï¼Œå¼‚å¸¸æ—¶é—´æ®µä¸ºå¼€å§‹æ—¶é—´å¾€å600ç§’
    anomaly_periods = []
    for _, row in label_df.iterrows():
        # å°†å¼€å§‹æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        st_time = pd.to_datetime(row['st_time']).timestamp() * 1000
        ed_time = st_time + 600 * 1000  # å¼€å§‹æ—¶é—´ + 600ç§’
        data_type = row.get('data_type', 'unknown')  # è·å–data_typeï¼Œé»˜è®¤ä¸ºunknown
        anomaly_periods.append((st_time, ed_time, data_type))
    
    print(f"å…±åŠ è½½ {len(anomaly_periods)} ä¸ªå¼‚å¸¸æ—¶é—´æ®µ")
    return anomaly_periods


def compute_normalization_stats(label_df):
    """
    ä»è®­ç»ƒé›†è®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯
    
    ç»Ÿè®¡æ–¹å¼:
        - Metric: æ’é™¤NaNï¼ŒåŒ…å«0ï¼ˆ0æ˜¯çœŸå®å€¼ï¼‰
        - Log: æ’é™¤0ï¼ˆ0æ˜¯çœŸå®çš„'æœªå‡ºç°'ï¼‰
        - Trace: æ’é™¤NaNå’Œ0ï¼ˆéƒ½æ˜¯ç¼ºå¤±ï¼‰ï¼Œä»…å¯¹Durationç»Ÿè®¡
    """
    train_df = label_df[label_df['data_type'] == 'train']
    print(f"\nä» {len(train_df)} ä¸ªè®­ç»ƒæ ·æœ¬è®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡...")
    
    # æŒ‰æŒ‡æ ‡åˆ†åˆ«æ”¶é›†
    all_metrics = [[] for _ in range(12)]  # 12ä¸ªmetricæŒ‡æ ‡
    all_logs = [[] for _ in range(48)]     # 48ä¸ªlogæ¨¡æ¿
    all_traces = [[] for _ in range(10)]   # 10ä¸ªinstance
    
    for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc="æ”¶é›†è®­ç»ƒæ•°æ®"):
        st_time = pd.to_datetime(row['st_time']).timestamp() * 1000
        ed_time = st_time + 600 * 1000
        
        # æ”¶é›†åŸå§‹æ•°æ®ï¼ˆä¸å½’ä¸€åŒ–ï¼‰
        metric, _ = _process_metric_for_sample(st_time, ed_time, normalize=False)
        log, _ = _process_log_for_sample(st_time, ed_time, normalize=False)
        trace, _ = _process_trace_for_sample(st_time, ed_time, normalize=False)
        
        # Metric: æŒ‰æŒ‡æ ‡æ”¶é›†éNaNå€¼ï¼ˆåŒ…å«0ï¼‰
        for i in range(12):
            vals = metric[:, :, i].flatten()
            valid_vals = vals[~np.isnan(vals)]  # æ’é™¤NaNï¼Œä¿ç•™0
            all_metrics[i].extend(valid_vals)
        
        # Log: æŒ‰æ¨¡æ¿æ”¶é›†é0å€¼
        for i in range(48):
            vals = log[:, i].flatten()
            non_zero_vals = vals[vals != 0]  # æ’é™¤0
            all_logs[i].extend(non_zero_vals)
        
        # Trace: æŒ‰instanceæ”¶é›†éNaNä¸”é0çš„å€¼ (åªæ”¶é›†é€šé“0: Duration)
        for i in range(10):
            vals = trace[i, :, 0]  # Channel 0: Duration
            valid_vals = vals[~np.isnan(vals) & (vals != 0)]  # æ’é™¤NaNå’Œ0
            all_traces[i].extend(valid_vals)
    
    # è®¡ç®—æ¯ä¸ªæŒ‡æ ‡çš„å‡å€¼å’Œæ ‡å‡†å·®
    print("\nè®¡ç®—ç»Ÿè®¡ä¿¡æ¯:")
    
    # Metricç»Ÿè®¡
    metric_means = np.zeros(12)
    metric_stds = np.zeros(12)
    for i in range(12):
        if len(all_metrics[i]) > 0:
            metric_means[i] = np.mean(all_metrics[i])
            metric_stds[i] = np.std(all_metrics[i])
            if metric_stds[i] == 0:
                metric_stds[i] = 1.0
            print(f"  Metric[{i}]: mean={metric_means[i]:.4f}, std={metric_stds[i]:.4f}, samples={len(all_metrics[i])}")
        else:
            metric_means[i] = 0.0
            metric_stds[i] = 1.0
            print(f"  Metric[{i}]: æ— æœ‰æ•ˆæ•°æ®")
    
    # Logç»Ÿè®¡
    log_means = np.zeros(48)
    log_stds = np.zeros(48)
    for i in range(48):
        if len(all_logs[i]) > 0:
            log_means[i] = np.mean(all_logs[i])
            log_stds[i] = np.std(all_logs[i])
            if log_stds[i] == 0:
                log_stds[i] = 1.0
        else:
            log_means[i] = 0.0
            log_stds[i] = 1.0
    print(f"  Log: {np.sum([len(all_logs[i]) > 0 for i in range(48)])}/48 ä¸ªæ¨¡æ¿æœ‰æ•°æ®")
    
    # Traceç»Ÿè®¡ (åªå¯¹Duration)
    trace_stats = []
    for i in range(10):
        if len(all_traces[i]) > 0:
            mean, std = np.mean(all_traces[i]), np.std(all_traces[i])
            trace_stats.append({'mean': mean, 'std': std if std > 0 else 1.0})
            print(f"  Trace[{SERVICES[i]}]: mean={mean:.4f}, std={std:.4f}, samples={len(all_traces[i])}")
        else:
            trace_stats.append({'mean': 0.0, 'std': 1.0})
            print(f"  Trace[{SERVICES[i]}]: æ— æœ‰æ•ˆæ•°æ®")
    
    metric_stats = {'mean': metric_means, 'std': metric_stds}
    log_stats = {'mean': log_means, 'std': log_stds}
    
    print("\nâœ… ç»Ÿè®¡ä¿¡æ¯è®¡ç®—å®Œæˆ")
    return {'metric': metric_stats, 'log': log_stats, 'trace': trace_stats}


def _process_metric_for_sample(st_time, ed_time, normalize=True):
    """
    å¤„ç†å•ä¸ªæ ·æœ¬çš„æŒ‡æ ‡æ•°æ®ï¼ˆä½¿ç”¨é¢„åŠ è½½çš„ç¼“å­˜ï¼‰
    
    Args:
        st_time: æ•…éšœå¼€å§‹æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        ed_time: æ•…éšœç»“æŸæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        normalize: æ˜¯å¦è¿›è¡Œå½’ä¸€åŒ–ï¼Œé»˜è®¤True
    
    Returns:
        tuple: (metric_data, availability)
            - metric_data: numpy array, shape [10, 20, 12]
            - availability: bool - æ•´ä¸ªmetricæ¨¡æ€æ˜¯å¦å¯ç”¨
    """    
    # ä½¿ç”¨å…¨å±€å®šä¹‰çš„æœåŠ¡é¡ºåº
    num_instances = len(SERVICES)
    
    # åˆå§‹åŒ–ç»“æœæ•°ç»„ [num_instances, 20 time_steps, 12 metrics]
    # åˆå§‹åŒ–ä¸ºNaNä»¥ä¾¿åç»­è¯†åˆ«ç¼ºå¤±
    metric_data = np.full((num_instances, 20, 12), np.nan)
    metric_names = None
    
    # æŒ‰ç…§å›ºå®šé¡ºåºéå†æ¯ä¸ªinstance
    for instance_idx, instance_name in enumerate(SERVICES):
        # ä»ç¼“å­˜ä¸­è¯»å–æ•°æ®
        if instance_name not in METRIC_DATA_CACHE:
            continue
        
        try:
            df = METRIC_DATA_CACHE[instance_name]
            mask = (df['timestamp'] >= st_time) & (df['timestamp'] <= ed_time)
            sample_data = df[mask].sort_values('timestamp')
            
            if metric_names is None:
                metric_names = [col for col in sample_data.columns if col != 'timestamp']
            
            # ä¸€æ¬¡æ€§èµ‹å€¼æ‰€æœ‰æŒ‡æ ‡æ•°æ®
            num_time_steps = min(len(sample_data), 20)
            if num_time_steps > 0:
                metric_data[instance_idx, :num_time_steps, :] = sample_data[metric_names].values[:num_time_steps]
        
        except Exception:
            continue
    
    # è®¡ç®—æ•´ä¸ªæ¨¡æ€çš„å¯ç”¨æ€§ï¼ˆå¦‚æœæ‰€æœ‰æ•°æ®éƒ½æ˜¯NaNï¼Œåˆ™æ•´ä¸ªæ¨¡æ€ä¸å¯ç”¨ï¼‰
    availability = not np.all(np.isnan(metric_data))
    
    # å¤„ç†å’Œå½’ä¸€åŒ–
    if normalize and NORMALIZATION_STATS['metric'] is not None:
        stats = NORMALIZATION_STATS['metric']
        
        # ç”¨å‡å€¼å¡«å……NaN
        for i in range(12):  # 12ä¸ªæŒ‡æ ‡
            nan_mask = np.isnan(metric_data[:, :, i])
            if nan_mask.any():
                metric_data[:, :, i][nan_mask] = stats['mean'][i]
        
        # å½’ä¸€åŒ–
        metric_data = (metric_data - stats['mean']) / stats['std']
    else:
        # å¦‚æœä¸å½’ä¸€åŒ–ï¼ˆç»Ÿè®¡é˜¶æ®µï¼‰ï¼Œå°†NaNæ›¿æ¢ä¸ºNaNä¿æŒåŸæ ·
        pass
    
    return metric_data, availability

def _process_log_for_sample(st_time, ed_time, normalize=True):
    """
    å¤„ç†å•ä¸ªæ ·æœ¬çš„logæ•°æ®ï¼ˆä½¿ç”¨é¢„åŠ è½½çš„ç¼“å­˜ï¼‰
    
    Args:
        st_time: æ•…éšœå¼€å§‹æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        ed_time: æ•…éšœç»“æŸæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        normalize: æ˜¯å¦è¿›è¡Œå½’ä¸€åŒ–ï¼Œé»˜è®¤True
    
    Returns:
        tuple: (log_data, availability)
            - log_data: numpy array, shape [10, 48]
            - availability: bool - æ•´ä¸ªlogæ¨¡æ€æ˜¯å¦å¯ç”¨
    """
    # ä½¿ç”¨å…¨å±€å®šä¹‰çš„æœåŠ¡é¡ºåº
    num_instances = len(SERVICES)
    num_templates = get_log_template_count('gaia')  # åŠ¨æ€è·å–æ¨¡æ¿æ•°é‡
    
    # åˆå§‹åŒ–ç»“æœæ•°ç»„ [num_instances, num_templates]
    log_data = np.zeros((num_instances, num_templates))
    
    # æŒ‰ç…§å›ºå®šé¡ºåºéå†æ¯ä¸ªinstance
    for instance_idx, instance_name in enumerate(SERVICES):
        # ä»ç¼“å­˜ä¸­è¯»å–æ•°æ®
        if instance_name not in LOG_DATA_CACHE:
            continue
        
        try:
            df = LOG_DATA_CACHE[instance_name]
            
            # ç­›é€‰æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
            mask = (df['timestamp_ts'] >= st_time) & (df['timestamp_ts'] <= ed_time)
            sample_data = df[mask]
            
            if len(sample_data) > 0:
                # ç»Ÿè®¡æ¯ä¸ªtemplate_idå‡ºç°çš„æ¬¡æ•°
                template_counts = sample_data['template_id'].value_counts()
                
                # å°†ç»Ÿè®¡ç»“æœå¡«å…¥å¯¹åº”ä½ç½®ï¼ˆtemplate_idä»1å¼€å§‹ï¼Œæ•°ç»„ç´¢å¼•ä»0å¼€å§‹ï¼‰
                for template_id, count in template_counts.items():
                    if 1 <= template_id <= num_templates:
                        log_data[instance_idx, template_id - 1] = count
        
        except Exception:
            continue
    
    # è®¡ç®—æ•´ä¸ªæ¨¡æ€çš„å¯ç”¨æ€§ï¼ˆå¦‚æœæ‰€æœ‰æ•°æ®éƒ½æ˜¯0ï¼Œåˆ™æ•´ä¸ªæ¨¡æ€ä¸å¯ç”¨ï¼‰
    availability = not np.all(log_data == 0)
    
    # å½’ä¸€åŒ–ï¼ˆä¸å¡«å……ï¼Œä¿æŒ0å€¼ï¼‰
    if normalize and NORMALIZATION_STATS['log'] is not None:
        stats = NORMALIZATION_STATS['log']
        log_data = (log_data - stats['mean']) / stats['std']
    
    return log_data, availability


def _process_trace_for_sample(st_time, ed_time, normalize=True):
    """
    å¤„ç†å•ä¸ªæ ·æœ¬çš„traceæ•°æ®ï¼ˆä½¿ç”¨é¢„åŠ è½½çš„ç¼“å­˜ï¼‰
    
    åŒé€šé“ç‰¹å¾æå–ï¼š
    - Channel 0: Duration (å“åº”æ—¶é—´)
    - Channel 1: Error Rate (é”™è¯¯ç‡, based on status_code >= 400)
    
    Args:
        st_time: æ•…éšœå¼€å§‹æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        ed_time: æ•…éšœç»“æŸæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        normalize: æ˜¯å¦è¿›è¡Œå½’ä¸€åŒ–ï¼Œé»˜è®¤True
    
    Returns:
        tuple: (trace_data, availability)
            - trace_data: numpy array, shape [10, 20, 2]
            - availability: bool - æ•´ä¸ªtraceæ¨¡æ€æ˜¯å¦å¯ç”¨
    """
    # ä½¿ç”¨å…¨å±€å®šä¹‰çš„æœåŠ¡é¡ºåº
    num_instances = len(SERVICES)
    num_time_segments = 20  # 20ä¸ªæ—¶é—´æ®µ
    segment_duration = 30 * 1000  # æ¯ä¸ªæ—¶é—´æ®µ30ç§’ï¼ˆæ¯«ç§’ï¼‰
    num_channels = 2 # Duration + ErrorRate
    
    # åˆå§‹åŒ–ç»“æœæ•°ç»„ [num_instances, num_time_segments, 2]ï¼Œé»˜è®¤å€¼ä¸ºNaN
    trace_data = np.full((num_instances, num_time_segments, num_channels), np.nan)
    
    # æŒ‰ç…§å›ºå®šé¡ºåºéå†æ¯ä¸ªinstance
    for instance_idx, instance_name in enumerate(SERVICES):
        # ä»ç¼“å­˜ä¸­è¯»å–æ•°æ®
        if instance_name not in TRACE_DATA_CACHE:
            continue
        
        try:
            df = TRACE_DATA_CACHE[instance_name]
            
            # ç­›é€‰æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
            mask = (df['start_time_ts'] >= st_time) & (df['start_time_ts'] <= ed_time)
            sample_data = df[mask]
            
            if len(sample_data) > 0:
                # å‘é‡åŒ–è®¡ç®—ï¼šæ‰¹é‡è®¡ç®—æ‰€æœ‰traceçš„æ—¶é—´æ®µç´¢å¼•
                timestamps = sample_data['start_time_ts'].values
                durations = sample_data['duration'].values
                status_codes = sample_data['status_code'].values
                
                # æ‰¹é‡è®¡ç®—æ—¶é—´åç§»å’Œæ®µç´¢å¼•
                time_offsets = timestamps - st_time
                segment_indices = (time_offsets // segment_duration).astype(int)
                
                # ç­›é€‰æœ‰æ•ˆçš„æ®µç´¢å¼•
                valid_mask = (segment_indices >= 0) & (segment_indices < num_time_segments)
                valid_segments = segment_indices[valid_mask]
                valid_durations = durations[valid_mask]
                valid_status = status_codes[valid_mask]
                
                # æŒ‰æ®µç´¢å¼•åˆ†ç»„è®¡ç®—
                for seg_idx in range(num_time_segments):
                    seg_mask = valid_segments == seg_idx
                    if seg_mask.any():
                        # 1. Duration Mean
                        mean_duration = valid_durations[seg_mask].mean()
                        trace_data[instance_idx, seg_idx, 0] = mean_duration
                        
                        # 2. Error Rate (status_code >= 400)
                        # è®¡ç®—é”™è¯¯è¯·æ±‚çš„æ¯”ä¾‹
                        seg_status = valid_status[seg_mask]
                        error_count = np.sum(seg_status >= 400)
                        error_rate = error_count / len(seg_status)
                        trace_data[instance_idx, seg_idx, 1] = error_rate
        
        except Exception:
            continue
    
    # è®¡ç®—æ•´ä¸ªæ¨¡æ€çš„å¯ç”¨æ€§ï¼ˆå¦‚æœæ‰€æœ‰æ•°æ®éƒ½æ˜¯NaNï¼Œåˆ™æ•´ä¸ªæ¨¡æ€ä¸å¯ç”¨ï¼‰
    availability = not np.all(np.isnan(trace_data))
    
    # å¤„ç†å’Œå½’ä¸€åŒ–
    if normalize and NORMALIZATION_STATS['trace'] is not None:
        for i in range(num_instances):
            stats = NORMALIZATION_STATS['trace'][i]
            
            # Channel 0 (Duration): ç”¨å‡å€¼å¡«å……NaNï¼Œç„¶åå½’ä¸€åŒ–
            nan_mask_0 = np.isnan(trace_data[i, :, 0])
            if nan_mask_0.any():
                trace_data[i, :, 0][nan_mask_0] = stats['mean']
            trace_data[i, :, 0] = (trace_data[i, :, 0] - stats['mean']) / stats['std']
            
            # Channel 1 (Error Rate): ç”¨0å¡«å……NaN (æ²¡æœ‰è¯·æ±‚å°±æ²¡æœ‰é”™è¯¯)ï¼Œä¸å½’ä¸€åŒ–(æœ¬èº«0-1)
            nan_mask_1 = np.isnan(trace_data[i, :, 1])
            if nan_mask_1.any():
                trace_data[i, :, 1][nan_mask_1] = 0.0
            # Error Rate ä¸éœ€è¦ Z-Score å½’ä¸€åŒ–
            
    else:
        # å¦‚æœä¸å½’ä¸€åŒ–ï¼ˆç»Ÿè®¡é˜¶æ®µï¼‰ï¼Œä¿æŒNaN
        pass
    
    return trace_data, availability


def _process_single_sample(row) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªæ•…éšœæ ·æœ¬
    """
    sample_id = row['index']
    fault_service = row['instance']
    fault_type = row['anomaly_type']
    st_time = pd.to_datetime(row['st_time']).timestamp() * 1000
    ed_time = st_time + 600 * 1000  # å¼€å§‹æ—¶é—´ + 600ç§’
    data_type = row['data_type']

    processed_sample = {
        'sample_id': sample_id,
        'fault_service': fault_service,
        'fault_type': fault_type,
        'st_time': st_time,
        'ed_time': ed_time,
        'data_type': data_type,
    }

    # å¤„ç†å„æ¨¡æ€æ•°æ®ï¼ˆè¿”å›æ•°æ®å’Œå¯ç”¨æ€§æ ‡è®°ï¼‰
    metric_data, metric_available = _process_metric_for_sample(st_time, ed_time)
    log_data, log_available = _process_log_for_sample(st_time, ed_time)
    trace_data, trace_available = _process_trace_for_sample(st_time, ed_time)
    
    processed_sample['metric_data'] = metric_data
    processed_sample['log_data'] = log_data
    processed_sample['trace_data'] = trace_data
    
    # æ·»åŠ å¯ç”¨æ€§æ ‡è®°ï¼ˆæ•´ä¸ªæ¨¡æ€çº§åˆ«ï¼‰
    processed_sample['metric_available'] = metric_available  # bool
    processed_sample['log_available'] = log_available        # bool
    processed_sample['trace_available'] = trace_available    # bool
    
    return processed_sample


def process_all_sample(label_df) -> Dict[int, Dict[str, Any]]:
    """
    å¤„ç†æ‰€æœ‰æ•…éšœæ ·æœ¬
    """
    processed_data = {}
    
    print(f"\nå¼€å§‹å¤„ç† {len(label_df)} ä¸ªæ•…éšœæ ·æœ¬...")
    
    for idx, row in tqdm(label_df.iterrows(), total=len(label_df), desc="Processing samples"):
        sample_id = row['index']
        try:
            processed_sample = _process_single_sample(row)
            processed_data[sample_id] = processed_sample
        except Exception as e:
            print(f"\nâŒ Failed to process sample {sample_id}: {e}")
            continue
    
    print(f"\nâœ… å®Œæˆï¼æˆåŠŸå¤„ç† {len(processed_data)}/{len(label_df)} ä¸ªæ ·æœ¬")
    return processed_data


if __name__ == "__main__":    
    label_file = os.path.join(_project_root, "data", "processed_data", "gaia", "label_gaia.csv")
    label_df = pd.read_csv(label_file)
    
    # 1. é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
    preload_all_data()
    
    # 2. è®¡ç®—æˆ–åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯
    stats_file = os.path.join(_project_root, "data", "processed_data", "gaia", "norm_stats.pkl")
    
    if os.path.exists(stats_file):
        print(f"\nğŸ“‚ åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡: {stats_file}")
        with open(stats_file, 'rb') as f:
            stats = pickle.load(f)
    else:
        print("\nğŸ”„ è®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡...")
        stats = compute_normalization_stats(label_df)
        with open(stats_file, 'wb') as f:
            pickle.dump(stats, f)
        print(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_file}")
    
    # è®¾ç½®å…¨å±€ç»Ÿè®¡ä¿¡æ¯
    NORMALIZATION_STATS['metric'] = stats['metric']
    NORMALIZATION_STATS['log'] = stats['log']
    NORMALIZATION_STATS['trace'] = stats['trace']
    
    # 3. å¤„ç†æ‰€æœ‰æ ·æœ¬
    processed_data = process_all_sample(label_df)
    
    # 4. ä¿å­˜å¤„ç†åçš„æ•°æ®
    output_file = os.path.join(_project_root, "data", "processed_data", "gaia", "dataset.pkl")
    with open(output_file, 'wb') as f:
        pickle.dump(processed_data, f)
    
    print(f"\nğŸ’¾ æ•°æ®é›†å·²ä¿å­˜: {output_file}")
    print(f"   - æ ·æœ¬æ•°: {len(processed_data)}")
    print(f"\næ•°æ®å¤„ç†ç­–ç•¥:")
    print(f"   Metric: æ’é™¤NaNç»Ÿè®¡ï¼ŒNaNå¡«å……ä¸ºå‡å€¼")
    print(f"   Log: æ’é™¤0ç»Ÿè®¡ï¼Œä¸å¡«å……ï¼ˆ0æ˜¯çœŸå®çš„'æœªå‡ºç°'ï¼‰")
    print(f"   Trace: åŒé€šé“ (Duration, ErrorRate)")
    print(f"     - Ch0(Duration): å½’ä¸€åŒ–ï¼ŒNaNå¡«å……å‡å€¼")
    print(f"     - Ch1(ErrorRate): ä¸å½’ä¸€åŒ–ï¼ŒNaNå¡«å……0")
    print(f"\nå¯ç”¨æ€§æ ‡è®°: æ¯ä¸ªæ ·æœ¬åŒ…å«æ¨¡æ€çº§åˆ«æ ‡è®°")
    print(f"   - metric_available: boolï¼ˆæ•´ä¸ªæ¨¡æ€æ˜¯å¦å¯ç”¨ï¼‰")
    print(f"   - log_available: boolï¼ˆæ•´ä¸ªæ¨¡æ€æ˜¯å¦å¯ç”¨ï¼‰")
    print(f"   - trace_available: boolï¼ˆæ•´ä¸ªæ¨¡æ€æ˜¯å¦å¯ç”¨ï¼‰")
