import os
import subprocess
import pandas as pd
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
import time
from datetime import datetime


project_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def copy_valid_metric_files() -> None:
    """
    è¯¥å‡½æ•°å°†åŸå§‹ metric æ–‡ä»¶å¤¹ï¼ˆpreprocess/raw_data/gaia/metricï¼‰ä¸‹åŒæ—¶åŒ…å«ç‰¹å®šæœåŠ¡åå’Œå…³é”®DockeræŒ‡æ ‡çš„æ–‡ä»¶ï¼Œ
    å¤åˆ¶åˆ°æœ¬æ¨¡å—ä¸‹çš„ processed_data æ–‡ä»¶å¤¹ä¸­ã€‚

    è¾“å…¥:
        æ— 

    è¿”å›å€¼ï¼š
        æ— 
    """
    # print(os.path.dirname(os.path.abspath(__file__)))
    
    metric_dir = os.path.join(project_dir, 'preprocess', 'raw_data', 'gaia', 'metric')
    # print(metric_dir)

    file_names = os.listdir(metric_dir)

    services = ['dbservice', 'mobservice', 'logservice', 'webservice', 'redisservice']
    
    docker_metrics = [
        "docker_cpu_total_norm_pct",    # æ€»CPUä½¿ç”¨ç‡
        # "docker_cpu_user_pct",          # ç”¨æˆ·æ€CPUä½¿ç”¨
        # "docker_cpu_kernel_pct",        # å†…æ ¸æ€CPUä½¿ç”¨
        "docker_memory_usage_pct",      # å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”
        # "docker_memory_usage_total",    # å†…å­˜ä½¿ç”¨æ€»é‡
        # "docker_memory_limit",          # å†…å­˜é™åˆ¶
        "docker_memory_fail_count",     # å†…å­˜åˆ†é…å¤±è´¥æ¬¡æ•°
        "docker_diskio_read_bytes",     # è¯»å–å­—èŠ‚æ•°
        "docker_diskio_write_bytes",    # å†™å…¥å­—èŠ‚æ•°
        # "docker_diskio_read_ops",       # è¯»æ“ä½œæ•°
        # "docker_diskio_write_ops",      # å†™æ“ä½œæ•°
        "docker_diskio_read_service_time",  # è¯»æœåŠ¡æ—¶é—´
        "docker_diskio_write_service_time", # å†™æœåŠ¡æ—¶é—´
        "docker_network_in_bytes",      # å…¥ç«™æµé‡
        "docker_network_out_bytes",     # å‡ºç«™æµé‡
        # "docker_network_in_packets",    # å…¥ç«™åŒ…æ•°
        # "docker_network_out_packets",   # å‡ºç«™åŒ…æ•°
        "docker_network_in_errors",     # å…¥ç«™é”™è¯¯
        "docker_network_out_errors",    # å‡ºç«™é”™è¯¯
        # "docker_network_in_dropped",    # å…¥ç«™ä¸¢åŒ…
        "docker_network_out_dropped",   # å‡ºç«™ä¸¢åŒ…
    ]
    
    # ç­›é€‰åŒæ—¶åŒ…å«æœåŠ¡åå’Œå…³é”®æŒ‡æ ‡çš„æ–‡ä»¶
    valid_files = []
    for fn in file_names:
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä¸€æœåŠ¡å
        has_service = any(s in fn for s in services)
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä¸€å…³é”®æŒ‡æ ‡
        has_metric = any(m in fn for m in docker_metrics)
        
        if has_service and has_metric:
            valid_files.append(fn)
    
    print(f"ç­›é€‰å‡º {len(valid_files)} ä¸ªåŒ…å«å…³é”®æŒ‡æ ‡çš„æ–‡ä»¶ï¼Œä»æ€»å…± {len(file_names)} ä¸ªæ–‡ä»¶ä¸­")
    
    # å°†ç¬¦åˆæ¡ä»¶çš„metricæ–‡ä»¶å¤åˆ¶åˆ°processed_dataä¸­
    for file in valid_files:
        file_path = os.path.join(metric_dir, file)
        processed_data_dir = os.path.join(project_dir, 'preprocess', 'processed_data', 'gaia', 'metric')
        if not os.path.exists(processed_data_dir):
            os.makedirs(processed_data_dir)
        target_path = os.path.join(processed_data_dir, file)
        subprocess.run(['cp', file_path, target_path])

def merge_date_range_files() -> None:
    """
    åˆå¹¶ç›¸åŒæœåŠ¡ã€ä¸»æœºã€æŒ‡æ ‡çš„ä¸åŒæ—¶é—´æ®µæ–‡ä»¶ã€‚
    å°† 2021-07-01_2021-07-15 å’Œ 2021-07-15_2021-07-31 åˆå¹¶æˆ 2021-07-01_2021-07-31
    
    è¾“å…¥:
        æ— 
    
    è¿”å›å€¼:
        æ— 
    """
    processed_data_dir = os.path.join(project_dir, 'preprocess', 'processed_data', 'gaia', 'metric')
    
    if not os.path.exists(processed_data_dir):
        print("processed_data ç›®å½•ä¸å­˜åœ¨")
        return
    
    # è·å–æ‰€æœ‰07-01åˆ°07-15çš„æ–‡ä»¶
    first_period_files = []
    for f in os.listdir(processed_data_dir):
        if "2021-07-01_2021-07-15" in f and f.endswith('.csv'):
            first_period_files.append(f)
    
    print(f"æ‰¾åˆ° {len(first_period_files)} ä¸ªç¬¬ä¸€æ—¶é—´æ®µçš„æ–‡ä»¶")
    
    merged_count = 0
    failed_count = 0
    
    for first_file in tqdm(first_period_files, desc="åˆå¹¶ä¸åŒæ—¶é—´æ®µçš„åŒä¸€æŒ‡æ ‡æ–‡ä»¶"):
        # æ„é€ å¯¹åº”çš„ç¬¬äºŒæ—¶é—´æ®µæ–‡ä»¶å
        second_file = first_file.replace("2021-07-01_2021-07-15", "2021-07-15_2021-07-31")
        merged_file = first_file.replace("2021-07-01_2021-07-15", "2021-07-01_2021-07-31")
        
        first_path = os.path.join(processed_data_dir, first_file)
        second_path = os.path.join(processed_data_dir, second_file)
        merged_path = os.path.join(processed_data_dir, merged_file)
        
        # æ£€æŸ¥ç¬¬äºŒä¸ªæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(second_path):
            print(f"è­¦å‘Šï¼šæ‰¾ä¸åˆ°å¯¹åº”çš„ç¬¬äºŒæ—¶é—´æ®µæ–‡ä»¶: {second_file}")
            failed_count += 1
            continue
        
        try:
            # è¯»å–ä¸¤ä¸ªCSVæ–‡ä»¶
            df1 = pd.read_csv(first_path)
            df2 = pd.read_csv(second_path)
            
            # åˆå¹¶æ•°æ®
            merged_df = pd.concat([df1, df2], ignore_index=True)
            
            # æŒ‰æ—¶é—´æˆ³æ’åº
            timestamp_col = 'timestamp'
            merged_df = merged_df.sort_values(timestamp_col).reset_index(drop=True)
            
            # å»é™¤é‡å¤æ—¶é—´æˆ³ï¼Œä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„è®°å½•
            merged_df = merged_df.drop_duplicates(subset=[timestamp_col], keep='first').reset_index(drop=True)
            
            # ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶
            merged_df.to_csv(merged_path, index=False)
            
            # åˆ é™¤åŸæ¥çš„ä¸¤ä¸ªæ–‡ä»¶
            os.remove(first_path)
            os.remove(second_path)
            
            merged_count += 1
            
        except Exception as e:
            print(f"åˆå¹¶æ–‡ä»¶ {first_file} æ—¶å‡ºé”™: {e}")
            failed_count += 1
            continue
    
    print(f"æˆåŠŸåˆå¹¶ {merged_count} å¯¹æ–‡ä»¶")
    print(f"å¤±è´¥ {failed_count} å¯¹æ–‡ä»¶")
    print(f"åˆå¹¶åæ–‡ä»¶æ€»æ•°: {len([f for f in os.listdir(processed_data_dir) if f.endswith('.csv')])}")

def merge_metrics_by_service_instance() -> None:
    """
    å°†ç›¸åŒæœåŠ¡å®ä¾‹çš„ä¸åŒæŒ‡æ ‡æ–‡ä»¶åˆå¹¶æˆå¤šåˆ—æ ¼å¼
    ä¾‹å¦‚ï¼š
    - dbservice1_0.0.0.4_docker_cpu_kernel_pct_2021-07-01_2021-07-31.csv
    - dbservice1_0.0.0.4_docker_cpu_user_pct_2021-07-01_2021-07-31.csv
    åˆå¹¶ä¸ºï¼š
    - dbservice1_0.0.0.4_2021-07-01_2021-07-31.csv (åŒ…å«å¤šä¸ªæŒ‡æ ‡åˆ—)
    
    è¾“å…¥:
        æ— 
    
    è¿”å›å€¼:
        æ— 
    """
    processed_data_dir = os.path.join(project_dir, 'preprocess', 'processed_data', 'gaia', 'metric')
    
    # è·å–æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = os.listdir(processed_data_dir)
    csv_files = sorted(csv_files)
    print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªéœ€è¦åˆå¹¶çš„æŒ‡æ ‡æ–‡ä»¶")
    
    # æŒ‰æœåŠ¡å®ä¾‹åˆ†ç»„æ–‡ä»¶
    service_groups = {}
    for filename in csv_files:
        # è§£ææ–‡ä»¶å: service_ip_metric_daterange.csv
        # ä¾‹å¦‚: dbservice1_0.0.0.4_docker_cpu_kernel_pct_2021-07-01_2021-07-31.csv
        
        # é€šè¿‡"_"åˆ†å‰²æ–‡ä»¶å
        splits = filename.replace('.csv', '').split('_')
        
        if len(splits) >= 6:  # è‡³å°‘éœ€è¦: service_ip_docker_metric_2021-07-01_2021-07-31
            # åªå–ç¬¬ä¸€ä¸ªéƒ¨åˆ†ä½œä¸ºæœåŠ¡å®ä¾‹: dbservice1
            service_instance = splits[0]
            
            # ä»ç¬¬4ä¸ªéƒ¨åˆ†å¼€å§‹åˆ°å€’æ•°ç¬¬3ä¸ªéƒ¨åˆ†æ˜¯æŒ‡æ ‡åç§°
            # ä¾‹å¦‚: splits = ['dbservice1', '0.0.0.4', 'docker', 'cpu', 'kernel', 'pct', '2021-07-01', '2021-07-31']
            # æŒ‡æ ‡åç§°åº”è¯¥æ˜¯: splits[3:len(splits)-2] = ['cpu', 'kernel', 'pct']
            metric_name = '_'.join(splits[3:len(splits)-2])
            
            if service_instance not in service_groups:
                service_groups[service_instance] = {}
            
            service_groups[service_instance][metric_name] = {
                'filename': filename
            }
        else:
            print(f"âš ï¸ æ— æ³•è§£ææ–‡ä»¶å: {filename} (åˆ†å‰²ååªæœ‰{len(splits)}ä¸ªéƒ¨åˆ†)")
    
    print(f"è¯†åˆ«å‡º {len(service_groups)} ä¸ªæœåŠ¡å®ä¾‹")
    
    # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡åç§°å¹¶æ’åºï¼Œç¡®ä¿æ‰€æœ‰instanceä½¿ç”¨ç›¸åŒçš„åˆ—é¡ºåº
    all_metrics = set()
    for metrics in service_groups.values():
        all_metrics.update(metrics.keys())
    sorted_metrics = sorted(all_metrics)
    print(f"è¯†åˆ«å‡º {len(sorted_metrics)} ä¸ªä¸åŒçš„æŒ‡æ ‡")
    print(f"æŒ‡æ ‡åˆ—è¡¨ï¼ˆæŒ‰å­—æ¯é¡ºåºï¼‰: {sorted_metrics}")
    
    merged_count = 0
    
    for service_instance, metrics in service_groups.items():
        
        print(f"ğŸ”„ åˆå¹¶ {service_instance}: {len(metrics)} ä¸ªæŒ‡æ ‡")
        
        try:
            merged_df = None
            
            for metric_name, info in metrics.items():
                file_path = os.path.join(processed_data_dir, info['filename'])
                df = pd.read_csv(file_path)
                
                # é‡å‘½åvalueåˆ—ä¸ºæŒ‡æ ‡åç§°
                df = df.rename(columns={'value': metric_name})
                
                if merged_df is None:
                    merged_df = df
                else:
                    # åŸºäºtimestampè¿›è¡Œå¤–è¿æ¥åˆå¹¶
                    merged_df = pd.merge(merged_df, df, on='timestamp', how='outer')
            
            # æŒ‰æ—¶é—´æˆ³æ’åº
            merged_df = merged_df.sort_values('timestamp').reset_index(drop=True)
            
            # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´ï¼štimestampåˆ—åœ¨å‰ï¼Œç„¶åæ˜¯æŒ‰å­—æ¯é¡ºåºæ’åˆ—çš„æŒ‡æ ‡åˆ—
            # åªä¿ç•™è¯¥instanceå®é™…æ‹¥æœ‰çš„æŒ‡æ ‡åˆ—
            available_metrics = [m for m in sorted_metrics if m in merged_df.columns]
            column_order = ['timestamp'] + available_metrics
            merged_df = merged_df[column_order]
            
            # ç”Ÿæˆåˆå¹¶åçš„æ–‡ä»¶å
            merged_filename = f"{service_instance}_metric.csv"
            merged_path = os.path.join(processed_data_dir, merged_filename)
            
            # ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶
            merged_df.to_csv(merged_path, index=False)
            
            # åˆ é™¤åŸå§‹çš„å•æŒ‡æ ‡æ–‡ä»¶
            for metric_name, info in metrics.items():
                original_path = os.path.join(processed_data_dir, info['filename'])
                os.remove(original_path)
            
            print(f"âœ… {service_instance}: åˆå¹¶å®Œæˆ -> {merged_filename}")
            print(f"   æŒ‡æ ‡åˆ—: {list(metrics.keys())}")
            print(f"   æ•°æ®è¡Œæ•°: {len(merged_df)}")
            
            merged_count += 1
            
        except Exception as e:
            print(f"âŒ åˆå¹¶ {service_instance} æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"\nåˆå¹¶å®Œæˆ:")
    print(f"  æˆåŠŸåˆå¹¶çš„æœåŠ¡å®ä¾‹: {merged_count}")
    print(f"  æœ€ç»ˆæ–‡ä»¶æ•°: {len([f for f in os.listdir(processed_data_dir) if f.endswith('.csv')])}")


def process_single_file_resample(file_path: str) -> tuple:
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶çš„30ç§’é—´éš”é‡é‡‡æ ·ï¼Œç›´æ¥è¦†ç›–åŸæ–‡ä»¶
    
    å‚æ•°:
        file_path: æ–‡ä»¶å®Œæ•´è·¯å¾„
    
    è¿”å›:
        tuple: (æ–‡ä»¶å, æ˜¯å¦æˆåŠŸ, åŸå§‹è¡Œæ•°, é‡é‡‡æ ·åè¡Œæ•°, é”™è¯¯ä¿¡æ¯æˆ–None)
    """
    try:
        filename = os.path.basename(file_path)
        df = pd.read_csv(file_path)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰timestampåˆ—
        if 'timestamp' not in df.columns:
            return (filename, False, 0, 0, "ç¼ºå°‘timestampåˆ—")
        
        original_rows = len(df)
        
        # è·å–æ•°æ®çš„æ—¶é—´èŒƒå›´
        min_timestamp = df['timestamp'].min()
        max_timestamp = df['timestamp'].max()
        
        # å›ºå®šçš„èµ·å§‹æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰å’Œé—´éš”
        start_timestamp = 1625133601000  # 2021-07-01 00:00:01 GMTï¼Œå›ºå®šèµ·å§‹æ—¶é—´
        interval_ms = 30 * 1000  # 30ç§’é—´éš”ï¼Œè½¬æ¢ä¸ºæ¯«ç§’
        
        # ä»å›ºå®šçš„èµ·å§‹æ—¶é—´æˆ³å¼€å§‹å¤„ç†
        current_timestamp = start_timestamp
        
        # å­˜å‚¨é‡é‡‡æ ·åçš„æ•°æ®
        resampled_data = []
        
        while current_timestamp <= max_timestamp:
            # å®šä¹‰30ç§’çª—å£çš„ç»“æŸæ—¶é—´
            window_end = current_timestamp + interval_ms
            
            # æŸ¥æ‰¾åœ¨å½“å‰30ç§’çª—å£å†…çš„æ•°æ®
            window_data = df[(df['timestamp'] >= current_timestamp) & 
                           (df['timestamp'] < window_end)]
            
            if not window_data.empty:
                # å¦‚æœæœ‰æ•°æ®ï¼Œå–ç¬¬ä¸€è¡Œï¼ˆæ—¶é—´æœ€æ—©çš„ï¼‰
                first_record = window_data.iloc[0].copy()
                # å°†æ—¶é—´æˆ³è®¾ç½®ä¸ºçª—å£èµ·å§‹æ—¶é—´
                first_record['timestamp'] = current_timestamp
                resampled_data.append(first_record)
            else:
                # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œåˆ›å»ºä¸€ä¸ªç©ºå€¼è®°å½•ï¼Œä¿ç•™æ—¶é—´æˆ³
                empty_record = pd.Series(index=df.columns)
                empty_record['timestamp'] = current_timestamp
                # å°†æ‰€æœ‰metricåˆ—è®¾ç½®ä¸ºç©ºå€¼ï¼ˆNaNï¼‰
                metric_columns = [col for col in df.columns if col != 'timestamp']
                for col in metric_columns:
                    empty_record[col] = pd.NA
                resampled_data.append(empty_record)
            
            # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ª30ç§’çª—å£
            current_timestamp += interval_ms
        
        if resampled_data:
            # åˆ›å»ºé‡é‡‡æ ·åçš„DataFrame
            resampled_df = pd.DataFrame(resampled_data)
            
            # ç›´æ¥è¦†ç›–åŸæ–‡ä»¶
            resampled_df.to_csv(file_path, index=False)
            
            resampled_rows = len(resampled_df)
            return (filename, True, original_rows, resampled_rows, None)
        else:
            return (filename, False, original_rows, 0, "åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰æ‰¾åˆ°æ•°æ®")
            
    except Exception as e:
        return (os.path.basename(file_path), False, 0, 0, str(e))


def resample_metrics_30s_interval(num_processes: int = None) -> None:
    """
    ä½¿ç”¨å¤šè¿›ç¨‹å¯¹processed_dataç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶è¿›è¡Œ30ç§’é—´éš”é‡é‡‡æ ·ï¼Œç›´æ¥è¦†ç›–åŸæ–‡ä»¶ã€‚
    ä»å›ºå®šæ—¶é—´æˆ³1625133601000å¼€å§‹ï¼Œæ¯30ç§’æŸ¥æ‰¾æ˜¯å¦æœ‰å€¼ã€‚
    å¦‚æœæœ‰å¤šä¸ªå€¼ï¼Œåªå–ç¬¬ä¸€ä¸ªå€¼ï¼Œå¹¶å°†å…¶æ”¾ç½®åˆ°è¯¥æ—¶é—´æˆ³å¤„ã€‚
    å¦‚æœæ²¡æœ‰æŒ‡æ ‡å€¼ï¼Œåˆ™ä¸è®°å½•è¯¥æ—¶é—´æˆ³ã€‚
    
    å‚æ•°:
        num_processes: ä½¿ç”¨çš„è¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
    
    è¿”å›å€¼:
        æ— 
    """
    processed_data_dir = os.path.join(project_dir, 'preprocess', 'processed_data', 'gaia', 'metric')
    
    # è·å–æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = [f for f in os.listdir(processed_data_dir) if f.endswith('.csv')]
    
    # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
    file_paths = [os.path.join(processed_data_dir, file) for file in csv_files]
    
    # ç¡®å®šè¿›ç¨‹æ•°
    if num_processes is None:
        num_processes = min(cpu_count(), len(file_paths))
    
    print(f"å¼€å§‹å¯¹ {len(file_paths)} ä¸ªæ–‡ä»¶è¿›è¡Œ30ç§’é—´éš”é‡é‡‡æ ·ï¼ˆè¦†ç›–åŸæ–‡ä»¶ï¼‰ï¼Œä½¿ç”¨ {num_processes} ä¸ªè¿›ç¨‹...")
    print(f"å›ºå®šèµ·å§‹æ—¶é—´æˆ³: 1625133601000 (æ¯«ç§’)")
    
    # ä½¿ç”¨å¤šè¿›ç¨‹æ± å¤„ç†æ–‡ä»¶
    with Pool(processes=num_processes) as pool:
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡
        results = list(tqdm(
            pool.imap(process_single_file_resample, file_paths),
            total=len(file_paths),
            desc="é‡é‡‡æ ·æ–‡ä»¶"
        ))
    
    # ç»Ÿè®¡ç»“æœ
    successful_count = 0
    failed_count = 0
    total_original_rows = 0
    total_resampled_rows = 0
    
    for filename, success, original_rows, resampled_rows, error in results:
        if success:
            successful_count += 1
            total_original_rows += original_rows
            total_resampled_rows += resampled_rows
        else:
            failed_count += 1
            if error:
                print(f"âŒ {filename}: {error}")
    
    print(f"\né‡é‡‡æ ·å®Œæˆ:")
    print(f"  æˆåŠŸå¤„ç†çš„æ–‡ä»¶: {successful_count}")
    print(f"  å¤±è´¥çš„æ–‡ä»¶: {failed_count}")
    print(f"  åŸå§‹æ•°æ®æ€»è¡Œæ•°: {total_original_rows:,}")
    print(f"  é‡é‡‡æ ·åæ€»è¡Œæ•°: {total_resampled_rows:,}")
    if total_original_rows > 0:
        print(f"  æ•°æ®å‹ç¼©ç‡: {((total_original_rows-total_resampled_rows)/total_original_rows*100):.1f}%")
    print(f"  é‡é‡‡æ ·é—´éš”: 30ç§’")
    print(f"  å›ºå®šèµ·å§‹æ—¶é—´æˆ³: 1625133601000 (æ¯«ç§’)")


def process_single_anomaly_sample_all_files(args: tuple) -> tuple:
    """
    å¤„ç†å•ä¸ªå¼‚å¸¸æ ·æœ¬ï¼Œä»æ‰€æœ‰10ä¸ªæ–‡ä»¶ä¸­æå–æ•…éšœæ—¶é—´çª—å£å†…çš„æ•°æ®
    åªè¦æœ‰ä¸€ä¸ªæ–‡ä»¶æœ‰æ•°æ®ï¼Œå°±ä¿ç•™è¯¥æ ·æœ¬çš„æ‰€æœ‰æ–‡ä»¶æ•°æ®
    
    å‚æ•°:
        args: (sample_idx, sample_row, metric_files_dict)
    
    è¿”å›:
        tuple: (sample_idx, success, error_msg, all_files_data_dict)
    """
    try:
        sample_idx, sample_row, metric_files_dict = args
        
        # è·å–æ ‡ç­¾ä¿¡æ¯
        service = sample_row['service']
        instance = sample_row['instance']
        st_time_str = sample_row['st_time']
        anomaly_type = sample_row['anomaly_type']
        
        # å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ¯«ç§’æ—¶é—´æˆ³
        dt = datetime.strptime(st_time_str, '%Y-%m-%d %H:%M:%S.%f')
        start_timestamp_ms = int(dt.timestamp() * 1000)
        end_timestamp_ms = start_timestamp_ms + 600 * 1000  # 600ç§’å
        
        # å­˜å‚¨æ‰€æœ‰æ–‡ä»¶çš„æ•°æ®å’Œæ—¶é—´æˆ³ä¿¡æ¯
        all_files_data = {}
        all_actual_timestamps = set()
        has_any_data = False
        
        # ç¬¬ä¸€è½®ï¼šæ£€æŸ¥æ‰€æœ‰æ–‡ä»¶ï¼Œæ”¶é›†æ—¶é—´æˆ³ä¿¡æ¯
        for file_key, metric_file_path in metric_files_dict.items():
            try:
                # è¯»å–æ–‡ä»¶
                df = pd.read_csv(metric_file_path)
                df.columns = df.columns.str.strip()
                
                # è·å–åˆ—åä¿¡æ¯ï¼ˆåªéœ€è¦ä¸€æ¬¡ï¼‰
                metric_columns = [col for col in df.columns if col != 'timestamp']
                
                # æå–æ—¶é—´çª—å£å†…çš„æ•°æ®
                window_data = df[(df['timestamp'] >= start_timestamp_ms) & 
                                (df['timestamp'] <= end_timestamp_ms)]
                
                if not window_data.empty:
                    has_any_data = True
                    actual_timestamps = sorted(window_data['timestamp'].unique())
                    all_actual_timestamps.update(actual_timestamps)
                    all_files_data[file_key] = {'metric_columns': metric_columns, 'window_data': window_data}
                else:
                    all_files_data[file_key] = {'metric_columns': metric_columns, 'window_data': pd.DataFrame()}
                    
            except Exception as e:
                # å¦‚æœæ–‡ä»¶è¯»å–å¤±è´¥ï¼Œåˆ›å»ºç©ºæ•°æ®ç»“æ„
                default_columns = ['docker_memory_fail_count', 'docker_network_out_dropped', 
                                 'docker_diskio_write_service_time', 'docker_diskio_write_bytes',
                                 'docker_diskio_read_service_time', 'docker_network_in_bytes',
                                 'docker_network_out_bytes', 'docker_diskio_read_bytes',
                                 'docker_network_in_errors', 'docker_network_out_errors',
                                 'docker_cpu_total_norm_pct', 'docker_memory_usage_pct']
                all_files_data[file_key] = {'metric_columns': default_columns, 'window_data': pd.DataFrame()}
        
        # å¦‚æœæ‰€æœ‰æ–‡ä»¶éƒ½æ²¡æœ‰æ•°æ®ï¼Œè·³è¿‡è¯¥æ ·æœ¬
        if not has_any_data:
            return (sample_idx, False, "æ‰€æœ‰æ–‡ä»¶åœ¨è¯¥æ—¶é—´çª—å£å†…éƒ½æ²¡æœ‰æ•°æ®", None)
        
        # è®¡ç®—ç»Ÿä¸€çš„æ—¶é—´æˆ³åºåˆ—
        if not all_actual_timestamps:
            return (sample_idx, False, "æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ—¶é—´æˆ³", None)
        
        # ä½¿ç”¨æœ€æ—©çš„æ—¶é—´æˆ³è®¡ç®—ç†æƒ³èµ·å§‹ç‚¹
        interval_ms = 30 * 1000
        reference_ts = min(all_actual_timestamps)
        
        # å¦‚æœç¬¬ä¸€ä¸ªå®é™…æ—¶é—´æˆ³ä¸æ•…éšœå¼€å§‹æ—¶é—´å·®>=30ç§’ï¼Œéœ€è¦å¾€å‰æ¨
        if (reference_ts - start_timestamp_ms) >= interval_ms:
            steps_back = (reference_ts - start_timestamp_ms) // interval_ms
            ideal_first_timestamp = reference_ts - steps_back * interval_ms
        else:
            ideal_first_timestamp = reference_ts
        
        # ç”Ÿæˆ20ä¸ªè¿ç»­æ—¶é—´æˆ³
        expected_timestamps = [ideal_first_timestamp + i * interval_ms for i in range(20)]
        
        # ç¬¬äºŒè½®ï¼šä¸ºæ¯ä¸ªæ–‡ä»¶ç”Ÿæˆå®Œæ•´æ•°æ®
        result_data = {}
        
        for file_key, file_data in all_files_data.items():
            metric_columns = file_data['metric_columns']
            window_data = file_data['window_data']
            
            # åˆ›å»ºè¯¥æ–‡ä»¶çš„å®Œæ•´æ•°æ®æ¡†æ¶
            complete_data = []
            
            for expected_ts in expected_timestamps:
                # æŸ¥æ‰¾è¯¥æ—¶é—´æˆ³çš„æ•°æ®
                if not window_data.empty:
                    exact_match = window_data[window_data['timestamp'] == expected_ts]
                else:
                    exact_match = pd.DataFrame()
                
                if not exact_match.empty:
                    # å¦‚æœæ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œä½¿ç”¨è¯¥æ•°æ®ï¼ˆåªä¿ç•™åŸå§‹metricæ•°æ®åˆ—ï¼‰
                    row_data = exact_match.iloc[0][['timestamp'] + metric_columns].to_dict()
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œåˆ›å»ºç©ºå€¼è¡Œ
                    row_data = {'timestamp': float(expected_ts)}
                    # å¯¹æ‰€æœ‰metricåˆ—è®¾ç½®ä¸ºNone
                    for col in metric_columns:
                        row_data[col] = None
                
                complete_data.append(row_data)
            
            # è½¬æ¢ä¸ºDataFrame
            complete_df = pd.DataFrame(complete_data)
            
            # ç¡®ä¿timestampåˆ—åœ¨ç¬¬ä¸€ä½
            cols = ['timestamp'] + [col for col in complete_df.columns if col != 'timestamp']
            complete_df = complete_df[cols]
            
            result_data[file_key] = complete_df
        
        return (sample_idx, True, None, result_data)
        
    except Exception as e:
        return (sample_idx, False, str(e), None)


def load_anomaly_periods(label_file_path):
    """
    åŠ è½½å¼‚å¸¸æ—¶é—´æ®µæ•°æ®ï¼ˆå›ºå®š600ç§’çª—å£ï¼‰
    
    Args:
        label_file_path (str): æ ‡ç­¾æ–‡ä»¶è·¯å¾„
        
    Returns:
        list: å¼‚å¸¸æ—¶é—´æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(start_timestamp, end_timestamp, data_type)çš„ä¸‰å…ƒç»„
    """
    print("æ­£åœ¨åŠ è½½å¼‚å¸¸æ—¶é—´æ®µæ•°æ®...")
    
    # è¯»å–æ ‡ç­¾æ–‡ä»¶
    label_df = pd.read_csv(label_file_path)
    
    # è½¬æ¢æ—¶é—´æ ¼å¼ä¸ºæ—¶é—´æˆ³ï¼Œå¼‚å¸¸æ—¶é—´æ®µä¸ºå¼€å§‹æ—¶é—´å¾€å600ç§’
    anomaly_periods = []
    for _, row in label_df.iterrows():
        # å°†å¼€å§‹æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        st_time = pd.to_datetime(row['st_time']).timestamp() * 1000
        ed_time = st_time + 600 * 1000  # å¼€å§‹æ—¶é—´ + 600ç§’
        data_type = row.get('data_type', 'unknown')  # è·å–data_typeï¼Œé»˜è®¤ä¸ºunknown
        anomaly_periods.append((st_time, ed_time, data_type))
    
    print(f"å…±åŠ è½½ {len(anomaly_periods)} ä¸ªå¼‚å¸¸æ—¶é—´æ®µ")
    return anomaly_periods


def create_selection_mask(times, target_periods):
    """
    åˆ›å»ºæŒ‡å®šæ—¶é—´æ®µçš„é€‰æ‹©æ©ç ï¼Œç”¨äºæ ‡è®°æ—¶é—´åºåˆ—ä¸­å±äºç›®æ ‡å‘¨æœŸçš„éƒ¨åˆ†
    
    Args:
        times (pd.Series): æ—¶é—´æˆ³ç³»åˆ—
        target_periods (list): ç›®æ ‡æ—¶é—´æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(start_time, end_time, ...)
        
    Returns:
        pd.Series: å¸ƒå°”æ©ç ï¼ŒTrueè¡¨ç¤ºå±äºç›®æ ‡å‘¨æœŸï¼ŒFalseè¡¨ç¤ºä¸å±äº
    """
    # åˆå§‹åŒ–æ‰€æœ‰æ—¶é—´ç‚¹ä¸ºæœªé€‰ä¸­ï¼ˆFalseï¼‰
    is_in_target = pd.Series(False, index=times.index)
    
    # å¯¹æ¯ä¸ªç›®æ ‡æ—¶é—´æ®µï¼Œæ ‡è®°å…¶ä¸­çš„æ—¶é—´ç‚¹ä¸ºé€‰ä¸­ï¼ˆTrueï¼‰
    for start_time, end_time, _ in target_periods:
        # æ‰¾å‡ºå½“å‰ç›®æ ‡æ—¶é—´æ®µå†…çš„æ‰€æœ‰æ—¶é—´ç‚¹
        in_current_period = (times >= start_time) & (times <= end_time)
        
        # å°†è¿™äº›æ—¶é—´ç‚¹æ ‡è®°ä¸ºé€‰ä¸­ï¼ˆTrueï¼‰
        is_in_target.loc[in_current_period] = True
    
    return is_in_target


def extract_anomaly_metric_data(metric_dir, anomaly_periods):
    """
    ä»processed_data/metricç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶æå–å¼‚å¸¸æ—¶é—´æ®µçš„metricæ•°æ®
    ç›´æ¥è¦†ç›–åŸå§‹æ–‡ä»¶
    
    Args:
        metric_dir (str): metricæ–‡ä»¶ç›®å½•
        anomaly_periods (list): å¼‚å¸¸æ—¶é—´æ®µåˆ—è¡¨
    """
    print("=== å¼€å§‹æå–å¼‚å¸¸metricæ•°æ®ï¼ˆè¦†ç›–åŸæ–‡ä»¶ï¼‰===")
    
    # è·å–æ‰€æœ‰metricæ–‡ä»¶
    metric_files = [f for f in os.listdir(metric_dir) if f.endswith('.csv')]
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶    
    for metric_file in metric_files:
        print(f"\nå¤„ç†æ–‡ä»¶: {metric_file}")
        metric_file_path = os.path.join(metric_dir, metric_file)
        
        # è¯»å–metricæ–‡ä»¶
        metric_df = pd.read_csv(metric_file_path)
        original_count = len(metric_df)
        print(f"  è¯»å–æ•°æ®: {original_count:,} æ¡")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰timestampåˆ—
        if 'timestamp' not in metric_df.columns:
            print(f"  âš ï¸ è·³è¿‡æ–‡ä»¶ {metric_file}: ç¼ºå°‘timestampåˆ—")
            continue
        
        # ä½¿ç”¨create_selection_maskå‡½æ•°åˆ›å»ºå¼‚å¸¸æ•°æ®æ©ç 
        anomaly_mask = create_selection_mask(metric_df['timestamp'], anomaly_periods)
        
        # æå–å¼‚å¸¸æ•°æ®
        anomaly_data = metric_df[anomaly_mask].copy()
        anomaly_count = len(anomaly_data)
        
        # ç›´æ¥è¦†ç›–åŸæ–‡ä»¶
        anomaly_data.to_csv(metric_file_path, index=False)
        
        print(f"  æå–å¼‚å¸¸æ•°æ®: {anomaly_count:,} æ¡ ({anomaly_count/original_count:.2%})")
        print(f"  å·²è¦†ç›–åŸæ–‡ä»¶: {metric_file_path}")
    
    print(f"\n=== å¼‚å¸¸metricæ•°æ®æå–å®Œæˆ ===")


def extract_anomaly_samples(num_processes: int = None) -> None:
    """
    å¹¶è¡Œæå–å¼‚å¸¸æ ·æœ¬çš„metricæ•°æ®
    æ ¹æ®æ ‡ç­¾æ–‡ä»¶ä¸­çš„æ•…éšœå¼€å§‹æ—¶é—´æˆ³ï¼Œæå–600ç§’æ—¶é—´çª—å£å†…çš„æ•°æ®
    
    å‚æ•°:
        num_processes: ä½¿ç”¨çš„è¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
    
    è¿”å›å€¼:
        æ— 
    """
    # æ–‡ä»¶è·¯å¾„
    label_file = os.path.join(project_dir, 'data', 'gaia', 'label.csv')
    processed_data_dir = os.path.join(project_dir, 'preprocess', 'processed_data', 'gaia')
    output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'anomaly_samples')
        
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"âœ… åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    # è¯»å–æ ‡ç­¾æ–‡ä»¶
    print("ğŸ“– è¯»å–æ ‡ç­¾æ–‡ä»¶...")
    labels_df = pd.read_csv(label_file)
    print(f"âœ… æˆåŠŸè¯»å– {len(labels_df)} ä¸ªå¼‚å¸¸æ ·æœ¬")
    
    # æ„å»ºmetricæ–‡ä»¶æ˜ å°„
    print("ğŸ” æ„å»ºmetricæ–‡ä»¶æ˜ å°„...")
    metric_files_dict = {}
    for filename in os.listdir(processed_data_dir):
        if filename.endswith('.csv'):
            # ä»æ–‡ä»¶åä¸­æå–æœåŠ¡å®ä¾‹å
            # ä¾‹å¦‚: dbservice1_0.0.0.4_2021-07-01_2021-07-31.csv -> dbservice1
            parts = filename.replace('.csv', '').split('_')
            if len(parts) >= 2:
                instance_name = parts[0]  # å–æœåŠ¡å®ä¾‹åä½œä¸ºé”®ï¼Œå¦‚ dbservice1, mobservice2
                metric_files_dict[instance_name] = os.path.join(processed_data_dir, filename)
    
    print(f"âœ… æ‰¾åˆ° {len(metric_files_dict)} ä¸ªmetricæ–‡ä»¶:")
    for instance, path in metric_files_dict.items():
        print(f"   {instance}: {os.path.basename(path)}")
    
    # å‡†å¤‡å¹¶è¡Œå¤„ç†çš„å‚æ•°
    process_args = []
    for idx, row in labels_df.iterrows():
        process_args.append((idx, row, metric_files_dict))
    
    # ç¡®å®šè¿›ç¨‹æ•°
    if num_processes is None:
        num_processes = min(cpu_count(), len(process_args))
    
    print(f"\nğŸš€ å¼€å§‹æå– {len(process_args)} ä¸ªå¼‚å¸¸æ ·æœ¬ï¼Œä½¿ç”¨ {num_processes} ä¸ªè¿›ç¨‹...")
    
    # ä½¿ç”¨å¤šè¿›ç¨‹æ± å¤„ç†
    with Pool(processes=num_processes) as pool:
        results = list(tqdm(
            pool.imap(process_single_anomaly_sample, process_args),
            total=len(process_args),
            desc="æå–å¼‚å¸¸æ ·æœ¬"
        ))
    
    # å¤„ç†ç»“æœå¹¶ä¿å­˜
    successful_samples = []
    failed_count = 0
    
    for sample_idx, success, error_msg, extracted_data in results:
        if success and extracted_data is not None:
            successful_samples.append(extracted_data)
        else:
            failed_count += 1
            if error_msg:
                print(f"âŒ æ ·æœ¬ {sample_idx}: {error_msg}")
    
    # åˆå¹¶æ‰€æœ‰æˆåŠŸæå–çš„æ•°æ®å¹¶ä¿å­˜
    if successful_samples:
        print(f"\nğŸ’¾ ä¿å­˜æå–çš„æ•°æ®...")
        all_samples_df = pd.concat(successful_samples, ignore_index=True)
        
        # ä¿å­˜å®Œæ•´æ•°æ®é›†
        all_samples_path = os.path.join(output_dir, 'all_anomaly_samples.csv')
        all_samples_df.to_csv(all_samples_path, index=False)
        print(f"âœ… å®Œæ•´æ•°æ®é›†å·²ä¿å­˜: {all_samples_path}")
        
        # æŒ‰æœåŠ¡åˆ†åˆ«ä¿å­˜
        for service in all_samples_df['service'].unique():
            service_data = all_samples_df[all_samples_df['service'] == service]
            service_path = os.path.join(output_dir, f'{service}_anomaly_samples.csv')
            service_data.to_csv(service_path, index=False)
            print(f"âœ… {service} æ•°æ®å·²ä¿å­˜: {service_path} ({len(service_data)} æ¡è®°å½•)")
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š æå–å®Œæˆç»Ÿè®¡:")
        print(f"  æˆåŠŸæå–çš„æ ·æœ¬: {len(successful_samples)}")
        print(f"  å¤±è´¥çš„æ ·æœ¬: {failed_count}")
        print(f"  æ€»æ•°æ®ç‚¹: {len(all_samples_df):,} (æ¯æ ·æœ¬20ä¸ªæ—¶é—´æˆ³)")
        print(f"  æ—¶é—´çª—å£é•¿åº¦: 600ç§’ (30ç§’é—´éš”)")
        print(f"  æ¶µç›–çš„æœåŠ¡: {', '.join(all_samples_df['service'].unique())}")
        print(f"  æ•°æ®åˆ—æ•°: {len(all_samples_df.columns)}")
        
        # ç»Ÿè®¡ç¼ºå¤±å€¼æƒ…å†µ
        total_metric_values = 0
        missing_metric_values = 0
        metric_columns = [col for col in all_samples_df.columns 
                         if col not in ['timestamp', 'sample_idx', 'service', 'instance', 'anomaly_type', 'start_timestamp']]
        
        for col in metric_columns:
            total_values = len(all_samples_df)
            # ç»Ÿè®¡ç©ºå€¼ï¼ˆNoneï¼‰å’ŒNaNå€¼
            missing_values = all_samples_df[col].isna().sum() + (all_samples_df[col] == '').sum()
            total_metric_values += total_values
            missing_metric_values += missing_values
        
        completion_rate = ((total_metric_values - missing_metric_values) / total_metric_values * 100) if total_metric_values > 0 else 0
        print(f"  æ•°æ®å®Œæ•´ç‡: {completion_rate:.1f}% ({total_metric_values - missing_metric_values:,}/{total_metric_values:,})")
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸæå–ä»»ä½•æ•°æ®æ ·æœ¬")


def extract_anomaly_samples_all_files(num_processes: int = None) -> None:
    """
    å¹¶è¡Œæå–å¼‚å¸¸æ ·æœ¬çš„metricæ•°æ®ï¼Œä»æ‰€æœ‰10ä¸ªæ–‡ä»¶ä¸­æå–æ•°æ®
    åªè¦æœ‰ä¸€ä¸ªæ–‡ä»¶æœ‰æ•°æ®å°±ä¿ç•™è¯¥æ ·æœ¬ï¼Œç”Ÿæˆ10ä¸ªå¯¹åº”çš„æ–°æ–‡ä»¶
    
    å‚æ•°:
        num_processes: ä½¿ç”¨çš„è¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
    
    è¿”å›å€¼:
        æ— 
    """
    # æ–‡ä»¶è·¯å¾„
    label_file = os.path.join(project_dir, 'preprocess', 'raw_data', 'gaia', 'label_gaia.csv')
    processed_data_dir = os.path.join(project_dir, 'preprocess', 'processed_data', 'gaia', 'metric')
    output_dir = os.path.join(project_dir, 'preprocess', 'processed_data', 'gaia', 'anomaly_metric')
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"âœ… åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    # è¯»å–æ ‡ç­¾æ–‡ä»¶
    print("ğŸ“– è¯»å–æ ‡ç­¾æ–‡ä»¶...")
    labels_df = pd.read_csv(label_file)
    print(f"âœ… æˆåŠŸè¯»å– {len(labels_df)} ä¸ªå¼‚å¸¸æ ·æœ¬")
    
    # æ„å»ºæ‰€æœ‰metricæ–‡ä»¶æ˜ å°„
    print("ğŸ” æ„å»ºæ‰€æœ‰metricæ–‡ä»¶æ˜ å°„...")
    metric_files_dict = {}
    for filename in os.listdir(processed_data_dir):
        if filename.endswith('.csv'):
            # ä½¿ç”¨å®Œæ•´æ–‡ä»¶åä½œä¸ºé”®ï¼Œä¾¿äºåç»­æŒ‰æ–‡ä»¶ä¿å­˜
            file_key = filename.replace('.csv', '')
            metric_files_dict[file_key] = os.path.join(processed_data_dir, filename)
    
    # å‡†å¤‡å¹¶è¡Œå¤„ç†çš„å‚æ•°
    process_args = []
    for idx, row in labels_df.iterrows():
        process_args.append((idx, row, metric_files_dict))
    
    # ç¡®å®šè¿›ç¨‹æ•°
    if num_processes is None:
        num_processes = min(cpu_count(), len(process_args))
    
    print(f"\nğŸš€ å¼€å§‹ä»æ‰€æœ‰æ–‡ä»¶ä¸­æå– {len(process_args)} ä¸ªå¼‚å¸¸æ ·æœ¬ï¼Œä½¿ç”¨ {num_processes} ä¸ªè¿›ç¨‹...")
    
    # ä½¿ç”¨å¤šè¿›ç¨‹æ± å¤„ç†
    with Pool(processes=num_processes) as pool:
        results = list(tqdm(
            pool.imap(process_single_anomaly_sample_all_files, process_args),
            total=len(process_args),
            desc="æå–å¼‚å¸¸æ ·æœ¬"
        ))
    
    # å¤„ç†ç»“æœå¹¶æŒ‰æ–‡ä»¶åˆ†åˆ«ä¿å­˜
    successful_samples = []
    failed_count = 0
    
    # åˆå§‹åŒ–æ¯ä¸ªæ–‡ä»¶çš„æ•°æ®åˆ—è¡¨
    file_data_dict = {file_key: [] for file_key in metric_files_dict.keys()}
    
    for sample_idx, success, error_msg, all_files_data in results:
        if success and all_files_data is not None:
            successful_samples.append(sample_idx)
            # å°†æ¯ä¸ªæ–‡ä»¶çš„æ•°æ®æ·»åŠ åˆ°å¯¹åº”åˆ—è¡¨ä¸­
            for file_key, file_df in all_files_data.items():
                file_data_dict[file_key].append(file_df)
        else:
            failed_count += 1
            if error_msg:
                print(f"âŒ æ ·æœ¬ {sample_idx}: {error_msg}")
    
    # åˆå¹¶å¹¶ä¿å­˜æ¯ä¸ªæ–‡ä»¶çš„æ•°æ®
    if successful_samples:
        print(f"\nğŸ’¾ ä¿å­˜æå–çš„æ•°æ®åˆ° anomaly_metric...")
        
        total_records = 0
        for file_key, data_list in file_data_dict.items():
            if data_list:  # å¦‚æœè¯¥æ–‡ä»¶æœ‰æ•°æ®
                # åˆå¹¶è¯¥æ–‡ä»¶çš„æ‰€æœ‰æ ·æœ¬æ•°æ®
                file_combined_df = pd.concat(data_list, ignore_index=True)
                
                # ä¿å­˜åˆ°processed_data2ç›®å½•
                output_filename = f"{file_key}_anomaly_samples.csv"
                output_path = os.path.join(output_dir, output_filename)
                file_combined_df.to_csv(output_path, index=False)
                
                total_records += len(file_combined_df)
                print(f"âœ… {file_key}: å·²ä¿å­˜ {len(file_combined_df)} æ¡è®°å½•")
            else:
                print(f"âš ï¸  {file_key}: æ²¡æœ‰æ•°æ®")
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š æå–å®Œæˆç»Ÿè®¡:")
        print(f"  æˆåŠŸæå–çš„æ ·æœ¬: {len(successful_samples)}")
        print(f"  å¤±è´¥çš„æ ·æœ¬: {failed_count}")
        print(f"  æ€»æ•°æ®è®°å½•: {total_records:,}")
        print(f"  ç”Ÿæˆæ–‡ä»¶æ•°: {len([f for f in file_data_dict.values() if f])}")
        print(f"  æ¯æ ·æœ¬æ—¶é—´æˆ³æ•°: 20ä¸ª (30ç§’é—´éš”)")
        print(f"  æ—¶é—´çª—å£é•¿åº¦: 600ç§’")
        print(f"  è¾“å‡ºç›®å½•: {output_dir}")
        
        # è®¡ç®—æ•°æ®å®Œæ•´ç‡ï¼ˆåŸºäºç¬¬ä¸€ä¸ªæœ‰æ•°æ®çš„æ–‡ä»¶ï¼‰
        sample_file_data = None
        for data_list in file_data_dict.values():
            if data_list:
                sample_file_data = pd.concat(data_list, ignore_index=True)
                break
        
        if sample_file_data is not None:
            metric_columns = [col for col in sample_file_data.columns 
                             if col not in ['timestamp', 'sample_idx', 'service', 'instance', 
                                           'anomaly_type', 'start_timestamp', 'file_source']]
            
            total_metric_values = len(sample_file_data) * len(metric_columns)
            missing_metric_values = 0
            for col in metric_columns:
                missing_values = sample_file_data[col].isna().sum()
                missing_metric_values += missing_values
            
            completion_rate = ((total_metric_values - missing_metric_values) / total_metric_values * 100) if total_metric_values > 0 else 0
            print(f"  æ•°æ®å®Œæ•´ç‡: {completion_rate:.1f}%")
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸæå–ä»»ä½•æ•°æ®æ ·æœ¬")


def remove_empty_samples_from_processed_data2() -> None:
    """
    ä»processed_data2ä¸­ç§»é™¤æ‰€æœ‰æ–‡ä»¶ä¸­éƒ½æ²¡æœ‰æ•°æ®çš„æ ·æœ¬
    åªæœ‰å½“ä¸€ä¸ªæ ‡ç­¾ï¼ˆæ ·æœ¬ï¼‰åœ¨æ‰€æœ‰10ä¸ªæ–‡ä»¶ä¸­çš„æ‰€æœ‰metricæ•°æ®éƒ½æ˜¯ç©ºå€¼æ—¶ï¼Œæ‰ç§»é™¤è¯¥æ ·æœ¬
    
    è¿”å›å€¼:
        æ— 
    """
    processed_data2_dir = os.path.join(project_dir, 'preprocess', 'processed_data', 'gaia', 'processed_data2')
    
    if not os.path.exists(processed_data2_dir):
        print(f"âŒ processed_data2 ç›®å½•ä¸å­˜åœ¨: {processed_data2_dir}")
        return
    
    # è·å–æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = [f for f in os.listdir(processed_data2_dir) if f.endswith('.csv')]
    
    if not csv_files:
        print("âŒ processed_data2 ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶")
        return
        
    print(f"ğŸ” å¼€å§‹æ¸…ç† {len(csv_files)} ä¸ªæ–‡ä»¶ä¸­çš„ç©ºæ ·æœ¬...")
    
    # ç¬¬ä¸€æ­¥ï¼šè¯»å–æ‰€æœ‰æ–‡ä»¶å¹¶ç¡®å®šæ ·æœ¬æ•°é‡
    all_files_data = {}
    sample_count = 0
    
    for filename in csv_files:
        file_path = os.path.join(processed_data2_dir, filename)
        
        try:
            df = pd.read_csv(file_path)
            if df.empty:
                print(f"âš ï¸  {filename}: æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡")
                continue
                
            all_files_data[filename] = df
            
            # ç¡®å®šæ ·æœ¬æ•°é‡ï¼ˆæ‰€æœ‰æ–‡ä»¶åº”è¯¥æœ‰ç›¸åŒçš„æ ·æœ¬æ•°ï¼‰
            current_sample_count = len(df) // 20 if len(df) % 20 == 0 else 0
            if sample_count == 0:
                sample_count = current_sample_count
            elif sample_count != current_sample_count:
                print(f"âš ï¸  {filename}: æ ·æœ¬æ•°é‡ä¸ä¸€è‡´ ({current_sample_count} vs {sample_count})")
                
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
            continue
    
    if not all_files_data:
        print("âŒ æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ–‡ä»¶")
        return
    
    print(f"ğŸ“Š æ£€æµ‹åˆ° {sample_count} ä¸ªæ ·æœ¬ï¼Œå¼€å§‹è·¨æ–‡ä»¶åˆ†æ...")
    
    # ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥æ¯ä¸ªæ ·æœ¬åœ¨æ‰€æœ‰æ–‡ä»¶ä¸­æ˜¯å¦éƒ½ä¸ºç©º
    samples_to_keep = []  # å­˜å‚¨è¦ä¿ç•™çš„æ ·æœ¬ç´¢å¼•
    
    for sample_idx in range(sample_count):
        has_any_data_across_files = False
        
        # æ£€æŸ¥è¯¥æ ·æœ¬åœ¨æ‰€æœ‰æ–‡ä»¶ä¸­æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªæ–‡ä»¶æœ‰æ•°æ®
        for filename, df in all_files_data.items():
            sample_start = sample_idx * 20
            sample_end = min(sample_start + 20, len(df))
            sample_data = df.iloc[sample_start:sample_end]
            
            # è·å–metricåˆ—ï¼ˆæ’é™¤timestampåˆ—ï¼‰
            metric_columns = [col for col in df.columns if col != 'timestamp']
            
            # æ£€æŸ¥è¯¥æ–‡ä»¶ä¸­çš„è¯¥æ ·æœ¬æ˜¯å¦æœ‰ä»»ä½•éç©ºæ•°æ®
            for col in metric_columns:
                non_null_values = sample_data[col].dropna()
                if len(non_null_values) > 0:
                    has_any_data_across_files = True
                    break
            
            if has_any_data_across_files:
                break
        
        if has_any_data_across_files:
            samples_to_keep.append(sample_idx)
    
    print(f"ğŸ“Š åˆ†æç»“æœ: {len(samples_to_keep)}/{sample_count} ä¸ªæ ·æœ¬æœ‰æ•°æ®ï¼Œå°†ç§»é™¤ {sample_count - len(samples_to_keep)} ä¸ªç©ºæ ·æœ¬")
    
    # ç¬¬ä¸‰æ­¥ï¼šä¸ºæ¯ä¸ªæ–‡ä»¶é‡æ–°ç”Ÿæˆæ•°æ®ï¼Œåªä¿ç•™æœ‰æ•ˆæ ·æœ¬
    total_removed_samples = sample_count - len(samples_to_keep)
    
    for filename, df in all_files_data.items():
        try:
            valid_samples = []
            
            for sample_idx in samples_to_keep:
                sample_start = sample_idx * 20
                sample_end = min(sample_start + 20, len(df))
                sample_data = df.iloc[sample_start:sample_end]
                valid_samples.append(sample_data)
            
            if valid_samples:
                # åˆå¹¶ä¿ç•™çš„æ ·æœ¬
                cleaned_df = pd.concat(valid_samples, ignore_index=True)
                
                # ä¿å­˜æ¸…ç†åçš„æ•°æ®
                file_path = os.path.join(processed_data2_dir, filename)
                cleaned_df.to_csv(file_path, index=False)
                
                final_sample_count = len(cleaned_df) // 20
                removed_count = sample_count - final_sample_count
                print(f"âœ… {filename}: åŸå§‹ {sample_count} æ ·æœ¬ -> ä¿ç•™ {final_sample_count} æ ·æœ¬ (ç§»é™¤ {removed_count} æ ·æœ¬)")
            else:
                # æ‰€æœ‰æ ·æœ¬éƒ½è¢«ç§»é™¤ï¼Œåˆ›å»ºç©ºæ–‡ä»¶
                empty_df = pd.DataFrame(columns=df.columns)
                file_path = os.path.join(processed_data2_dir, filename)
                empty_df.to_csv(file_path, index=False)
                print(f"ğŸ—‘ï¸  {filename}: æ‰€æœ‰æ ·æœ¬éƒ½è¢«ç§»é™¤ï¼Œæ–‡ä»¶å·²æ¸…ç©º")
                
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"\nğŸ“Š æ¸…ç†å®Œæˆç»Ÿè®¡:")
    print(f"  åŸå§‹æ ·æœ¬æ€»æ•°: {sample_count}")
    print(f"  ç§»é™¤çš„ç©ºæ ·æœ¬: {total_removed_samples}")
    print(f"  ä¿ç•™çš„æœ‰æ•ˆæ ·æœ¬: {len(samples_to_keep)}")
    print(f"  æ¸…ç†æ¯”ä¾‹: {(total_removed_samples/sample_count*100):.1f}%" if sample_count > 0 else "  æ¸…ç†æ¯”ä¾‹: 0%")


def keep_only_complete_samples_from_processed_data2() -> None:
    """
    ä»processed_data2ä¸­åªä¿ç•™æ‰€æœ‰æ–‡ä»¶ä¸­æ‰€æœ‰æŒ‡æ ‡éƒ½æ²¡æœ‰ç©ºå€¼çš„æ ·æœ¬
    ç§»é™¤ä»»ä½•åœ¨ä»»ä¸€æ–‡ä»¶ä¸­æœ‰ä»»ä½•æŒ‡æ ‡ä¸ºç©ºå€¼çš„æ ·æœ¬ï¼Œåªä¿ç•™å®Œå…¨å®Œæ•´çš„æ•°æ®
    
    è¿”å›å€¼:
        æ— 
    """
    processed_data2_dir = os.path.join(project_dir, 'preprocess', 'processed_data', 'gaia', 'processed_data2')
    
    if not os.path.exists(processed_data2_dir):
        print(f"âŒ processed_data2 ç›®å½•ä¸å­˜åœ¨: {processed_data2_dir}")
        return
    
    # è·å–æ‰€æœ‰CSVæ–‡ä»¶
    csv_files = [f for f in os.listdir(processed_data2_dir) if f.endswith('.csv')]
    
    if not csv_files:
        print("âŒ processed_data2 ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶")
        return
        
    print(f"ğŸ” å¼€å§‹ç­›é€‰ {len(csv_files)} ä¸ªæ–‡ä»¶ä¸­çš„å®Œæ•´æ ·æœ¬...")
    
    # ç¬¬ä¸€æ­¥ï¼šè¯»å–æ‰€æœ‰æ–‡ä»¶å¹¶ç¡®å®šæ ·æœ¬æ•°é‡
    all_files_data = {}
    sample_count = 0
    
    for filename in csv_files:
        file_path = os.path.join(processed_data2_dir, filename)
        
        try:
            df = pd.read_csv(file_path)
            if df.empty:
                print(f"âš ï¸  {filename}: æ–‡ä»¶ä¸ºç©ºï¼Œè·³è¿‡")
                continue
                
            all_files_data[filename] = df
            
            # ç¡®å®šæ ·æœ¬æ•°é‡ï¼ˆæ‰€æœ‰æ–‡ä»¶åº”è¯¥æœ‰ç›¸åŒçš„æ ·æœ¬æ•°ï¼‰
            current_sample_count = len(df) // 20 if len(df) % 20 == 0 else 0
            if sample_count == 0:
                sample_count = current_sample_count
            elif sample_count != current_sample_count:
                print(f"âš ï¸  {filename}: æ ·æœ¬æ•°é‡ä¸ä¸€è‡´ ({current_sample_count} vs {sample_count})")
                
        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
            continue
    
    if not all_files_data:
        print("âŒ æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•æ–‡ä»¶")
        return
    
    print(f"ğŸ“Š æ£€æµ‹åˆ° {sample_count} ä¸ªæ ·æœ¬ï¼Œå¼€å§‹è·¨æ–‡ä»¶å®Œæ•´æ€§åˆ†æ...")
    
    # ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥æ¯ä¸ªæ ·æœ¬åœ¨æ‰€æœ‰æ–‡ä»¶ä¸­æ˜¯å¦éƒ½æ²¡æœ‰ç©ºå€¼
    samples_to_keep = []  # å­˜å‚¨è¦ä¿ç•™çš„æ ·æœ¬ç´¢å¼•
    
    for sample_idx in range(sample_count):
        is_complete_across_all_files = True
        
        # æ£€æŸ¥è¯¥æ ·æœ¬åœ¨æ‰€æœ‰æ–‡ä»¶ä¸­æ˜¯å¦éƒ½å®Œå…¨æ²¡æœ‰ç©ºå€¼
        for filename, df in all_files_data.items():
            sample_start = sample_idx * 20
            sample_end = min(sample_start + 20, len(df))
            sample_data = df.iloc[sample_start:sample_end]
            
            # è·å–metricåˆ—ï¼ˆæ’é™¤timestampåˆ—ï¼‰
            metric_columns = [col for col in df.columns if col != 'timestamp']
            
            # æ£€æŸ¥è¯¥æ–‡ä»¶ä¸­çš„è¯¥æ ·æœ¬æ˜¯å¦æœ‰ä»»ä½•ç©ºå€¼
            for col in metric_columns:
                null_count = sample_data[col].isna().sum()
                if null_count > 0:
                    # å‘ç°ç©ºå€¼ï¼Œè¯¥æ ·æœ¬ä¸å®Œæ•´
                    is_complete_across_all_files = False
                    break
            
            if not is_complete_across_all_files:
                break
        
        if is_complete_across_all_files:
            samples_to_keep.append(sample_idx)
    
    print(f"ğŸ“Š åˆ†æç»“æœ: {len(samples_to_keep)}/{sample_count} ä¸ªæ ·æœ¬å®Œå…¨æ— ç©ºå€¼ï¼Œå°†ç§»é™¤ {sample_count - len(samples_to_keep)} ä¸ªä¸å®Œæ•´æ ·æœ¬")
    
    # ç¬¬ä¸‰æ­¥ï¼šä¸ºæ¯ä¸ªæ–‡ä»¶é‡æ–°ç”Ÿæˆæ•°æ®ï¼Œåªä¿ç•™å®Œæ•´æ ·æœ¬
    total_removed_samples = sample_count - len(samples_to_keep)
    
    for filename, df in all_files_data.items():
        try:
            complete_samples = []
            
            for sample_idx in samples_to_keep:
                sample_start = sample_idx * 20
                sample_end = min(sample_start + 20, len(df))
                sample_data = df.iloc[sample_start:sample_end]
                complete_samples.append(sample_data)
            
            if complete_samples:
                # åˆå¹¶ä¿ç•™çš„å®Œæ•´æ ·æœ¬
                cleaned_df = pd.concat(complete_samples, ignore_index=True)
                
                # ä¿å­˜ç­›é€‰åçš„æ•°æ®
                file_path = os.path.join(processed_data2_dir, filename)
                cleaned_df.to_csv(file_path, index=False)
                
                final_sample_count = len(cleaned_df) // 20
                removed_count = sample_count - final_sample_count
                print(f"âœ… {filename}: åŸå§‹ {sample_count} æ ·æœ¬ -> ä¿ç•™ {final_sample_count} å®Œæ•´æ ·æœ¬ (ç§»é™¤ {removed_count} ä¸å®Œæ•´æ ·æœ¬)")
            else:
                # æ²¡æœ‰å®Œæ•´æ ·æœ¬ï¼Œåˆ›å»ºç©ºæ–‡ä»¶
                empty_df = pd.DataFrame(columns=df.columns)
                file_path = os.path.join(processed_data2_dir, filename)
                empty_df.to_csv(file_path, index=False)
                print(f"ğŸ—‘ï¸  {filename}: æ²¡æœ‰å®Œæ•´æ ·æœ¬ï¼Œæ–‡ä»¶å·²æ¸…ç©º")
                
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"\nğŸ“Š å®Œæ•´æ€§ç­›é€‰å®Œæˆç»Ÿè®¡:")
    print(f"  åŸå§‹æ ·æœ¬æ€»æ•°: {sample_count}")
    print(f"  ç§»é™¤çš„ä¸å®Œæ•´æ ·æœ¬: {total_removed_samples}")
    print(f"  ä¿ç•™çš„å®Œæ•´æ ·æœ¬: {len(samples_to_keep)}")
    print(f"  ç­›é€‰æ¯”ä¾‹: {(total_removed_samples/sample_count*100):.1f}%" if sample_count > 0 else "  ç­›é€‰æ¯”ä¾‹: 0%")
    print(f"  æ•°æ®å®Œæ•´ç‡: 100.0% (æ‰€æœ‰ä¿ç•™æ ·æœ¬éƒ½æ²¡æœ‰ç©ºå€¼)")


if __name__ == "__main__":
    project_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    label_file = os.path.join(project_dir, 'preprocess', 'raw_data', 'gaia', 'label_gaia.csv')
    metric_dir = os.path.join(project_dir, 'preprocess', 'processed_data', 'gaia', 'metric')
    
    # å¤åˆ¶é€‰å®šmetricæ–‡ä»¶åˆ°processed_data/metricç›®å½•ä¸‹
    copy_valid_metric_files()

    # åˆå¹¶ä¸åŒæ—¶é—´æ®µçš„åŒä¸€æŒ‡æ ‡æ–‡ä»¶
    merge_date_range_files()

    # é‡é‡‡æ ·30ç§’é—´éš”çš„æŒ‡æ ‡æ•°æ®
    resample_metrics_30s_interval()

    # åˆå¹¶ç›¸åŒæœåŠ¡å®ä¾‹çš„ä¸åŒæŒ‡æ ‡æ–‡ä»¶
    merge_metrics_by_service_instance()

    # åŠ è½½å¼‚å¸¸æ—¶é—´æ®µ
    anomaly_periods = load_anomaly_periods(label_file)
    
    # æå–å¼‚å¸¸æ—¶é—´æ®µçš„metricæ•°æ®ï¼ˆç›´æ¥è¦†ç›–åŸæ–‡ä»¶ï¼‰
    extract_anomaly_metric_data(metric_dir, anomaly_periods)
    

    """
    # æ¸…ç†æ ·æœ¬ (äºŒé€‰ä¸€)
    # remove_empty_samples_from_processed_data2()        # å®½æ¾æ¸…ç†ï¼šåªç§»é™¤æ‰€æœ‰æ–‡ä»¶ä¸­éƒ½å®Œå…¨ä¸ºç©ºçš„æ ·æœ¬
    # keep_only_complete_samples_from_processed_data2()   # ä¸¥æ ¼æ¸…ç†ï¼šåªä¿ç•™æ‰€æœ‰æ–‡ä»¶ä¸­éƒ½å®Œå…¨æ²¡æœ‰ç©ºå€¼çš„æ ·æœ¬
    """