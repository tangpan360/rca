import os
import numpy as np
import pandas as pd
from tqdm import tqdm
from typing import Dict, Any

# å…¨å±€å®šä¹‰ï¼šæ‰€æœ‰instanceçš„å›ºå®šé¡ºåºï¼ˆç”¨äºæ‰€æœ‰æ¨¡æ€ï¼‰
SERVICES = ['dbservice1', 'dbservice2', 'logservice1', 'logservice2', 
            'mobservice1', 'mobservice2', 'redisservice1', 'redisservice2', 
            'webservice1', 'webservice2']

# å…¨å±€ç¼“å­˜ï¼šé¢„åŠ è½½çš„æ•°æ®
METRIC_DATA_CACHE = {}
LOG_DATA_CACHE = {}
TRACE_DATA_CACHE = {}

# å…¨å±€å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»è®­ç»ƒé›†è®¡ç®—ï¼‰
NORMALIZATION_STATS = {
    'metric': None,  # {'mean': [12], 'std': [12]}
    'log': None,     # {'mean': [40], 'std': [40]}
    'trace': None    # [{'mean': float, 'std': float}] * 10
}


def preload_all_data():
    """
    é¢„åŠ è½½æ‰€æœ‰æ¨¡æ€çš„æ•°æ®åˆ°å†…å­˜
    """
    project_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    
    print("=" * 50)
    print("å¼€å§‹é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜...")
    print("=" * 50)
    
    # 1. é¢„åŠ è½½ Metric æ•°æ®
    print("\n[1/3] åŠ è½½ Metric æ•°æ®...")
    metric_data_dir = os.path.join(project_dir, 'preprocess', 'processed_data', 'metric')
    for instance_name in tqdm(SERVICES, desc="Metric"):
        metric_file = os.path.join(metric_data_dir, f"{instance_name}_metric.csv")
        if os.path.exists(metric_file):
            df = pd.read_csv(metric_file)
            METRIC_DATA_CACHE[instance_name] = df
            print(f"  âœ“ {instance_name}: {len(df)} è¡Œæ•°æ®")
    
    # 2. é¢„åŠ è½½ Log æ•°æ®
    print("\n[2/3] åŠ è½½ Log æ•°æ®...")
    log_data_dir = os.path.join(project_dir, 'preprocess', 'processed_data', 'log')
    for instance_name in tqdm(SERVICES, desc="Log"):
        log_file = os.path.join(log_data_dir, f"{instance_name}_log.csv")
        if os.path.exists(log_file):
            # åªè¯»å–éœ€è¦çš„åˆ—ä»¥èŠ‚çœå†…å­˜
            df = pd.read_csv(log_file, usecols=['timestamp_ts', 'template_id'])
            LOG_DATA_CACHE[instance_name] = df
            print(f"  âœ“ {instance_name}: {len(df)} è¡Œæ•°æ®")
    
    # 3. é¢„åŠ è½½ Trace æ•°æ®
    print("\n[3/3] åŠ è½½ Trace æ•°æ®...")
    trace_data_dir = os.path.join(project_dir, 'preprocess', 'processed_data', 'trace')
    for instance_name in tqdm(SERVICES, desc="Trace"):
        trace_file = os.path.join(trace_data_dir, f"{instance_name}_trace.csv")
        if os.path.exists(trace_file):
            # åªè¯»å–éœ€è¦çš„åˆ—ä»¥èŠ‚çœå†…å­˜
            df = pd.read_csv(trace_file, usecols=['start_time_ts', 'duration'])
            TRACE_DATA_CACHE[instance_name] = df
            print(f"  âœ“ {instance_name}: {len(df)} è¡Œæ•°æ®")
    
    print("\n" + "=" * 50)
    print("æ•°æ®é¢„åŠ è½½å®Œæˆï¼")
    print(f"  Metric: {len(METRIC_DATA_CACHE)} ä¸ªå®ä¾‹")
    print(f"  Log: {len(LOG_DATA_CACHE)} ä¸ªå®ä¾‹")
    print(f"  Trace: {len(TRACE_DATA_CACHE)} ä¸ªå®ä¾‹")
    print("=" * 50 + "\n")


def load_anomaly_periods(label_file_path):
    """
    åŠ è½½å¼‚å¸¸æ—¶é—´æ®µæ•°æ®ï¼ˆå›ºå®š600ç§’çª—å£ï¼‰
    
    Args:
        label_file_path (str): æ ‡ç­¾æ–‡ä»¶è·¯å¾„
        
    Returns:
        list: å¼‚å¸¸æ—¶é—´æ®µåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(start_timestamp, end_timestamp, data_type)çš„ä¸‰å…ƒç»„
    """
    print("æ­£åœ¨åŠ è½½å¼‚å¸¸æ—¶é—´æ®µæ•°æ®...")
    
    # è¯»å–æ ‡ç­¾æ–‡ä»¶
    label_df = pd.read_csv(label_file_path)
    
    # è½¬æ¢æ—¶é—´æ ¼å¼ä¸ºæ—¶é—´æˆ³ï¼Œå¼‚å¸¸æ—¶é—´æ®µä¸ºå¼€å§‹æ—¶é—´å¾€å600ç§’
    anomaly_periods = []
    for _, row in label_df.iterrows():
        # å°†å¼€å§‹æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        st_time = pd.to_datetime(row['st_time']).timestamp() * 1000
        ed_time = st_time + 600 * 1000  # å¼€å§‹æ—¶é—´ + 600ç§’
        data_type = row.get('data_type', 'unknown')  # è·å–data_typeï¼Œé»˜è®¤ä¸ºunknown
        anomaly_periods.append((st_time, ed_time, data_type))
    
    print(f"å…±åŠ è½½ {len(anomaly_periods)} ä¸ªå¼‚å¸¸æ—¶é—´æ®µ")
    return anomaly_periods


def compute_normalization_stats(label_df):
    """
    ä»è®­ç»ƒé›†è®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯
    """
    train_df = label_df[label_df['data_type'] == 'train']
    print(f"\nä» {len(train_df)} ä¸ªè®­ç»ƒæ ·æœ¬è®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡...")
    
    all_metrics, all_logs, all_traces = [], [], [[] for _ in range(10)]
    
    for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc="æ”¶é›†è®­ç»ƒæ•°æ®"):
        st_time = pd.to_datetime(row['st_time']).timestamp() * 1000
        ed_time = st_time + 600 * 1000
        
        # æ”¶é›†åŸå§‹æ•°æ®
        metric = _process_metric_for_sample(st_time, ed_time)
        log = _process_log_for_sample(st_time, ed_time)
        trace = _process_trace_for_sample(st_time, ed_time)
        
        all_metrics.append(metric.reshape(-1, 12))
        all_logs.append(log.reshape(-1, 40))
        
        for i in range(10):
            vals = trace[i, :, 0]
            all_traces[i].extend(vals[vals != 0])
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    all_metrics = np.vstack(all_metrics)
    all_logs = np.vstack(all_logs)
    
    metric_stats = {
        'mean': np.mean(all_metrics, axis=0),
        'std': np.std(all_metrics, axis=0)
    }
    metric_stats['std'][metric_stats['std'] == 0] = 1.0
    
    log_stats = {
        'mean': np.mean(all_logs, axis=0),
        'std': np.std(all_logs, axis=0)
    }
    log_stats['std'][log_stats['std'] == 0] = 1.0
    
    trace_stats = []
    for i in range(10):
        if len(all_traces[i]) > 0:
            mean, std = np.mean(all_traces[i]), np.std(all_traces[i])
            trace_stats.append({'mean': mean, 'std': std if std > 0 else 1.0})
        else:
            trace_stats.append({'mean': 0.0, 'std': 1.0})
    
    print("âœ… ç»Ÿè®¡ä¿¡æ¯è®¡ç®—å®Œæˆ")
    return {'metric': metric_stats, 'log': log_stats, 'trace': trace_stats}


def _process_metric_for_sample(st_time, ed_time) -> np.ndarray:
    """
    å¤„ç†å•ä¸ªæ ·æœ¬çš„æŒ‡æ ‡æ•°æ®ï¼ˆä½¿ç”¨é¢„åŠ è½½çš„ç¼“å­˜ï¼‰
    
    Args:
        st_time: æ•…éšœå¼€å§‹æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        ed_time: æ•…éšœç»“æŸæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
    
    Returns:
        numpy array, shape [10, 20, 12] - 10ä¸ªinstanceï¼Œ20ä¸ªæ—¶é—´æ­¥ï¼Œ12ä¸ªæŒ‡æ ‡
    """    
    # ä½¿ç”¨å…¨å±€å®šä¹‰çš„æœåŠ¡é¡ºåº
    num_instances = len(SERVICES)
    
    # åˆå§‹åŒ–ç»“æœæ•°ç»„ [num_instances, 20 time_steps, 12 metrics]
    metric_data = np.zeros((num_instances, 20, 12))
    metric_names = None
    
    # æŒ‰ç…§å›ºå®šé¡ºåºéå†æ¯ä¸ªinstance
    for instance_idx, instance_name in enumerate(SERVICES):
        # ä»ç¼“å­˜ä¸­è¯»å–æ•°æ®
        if instance_name not in METRIC_DATA_CACHE:
            continue
        
        try:
            df = METRIC_DATA_CACHE[instance_name]
            mask = (df['timestamp'] >= st_time) & (df['timestamp'] <= ed_time)
            sample_data = df[mask].sort_values('timestamp')
            
            if metric_names is None:
                metric_names = [col for col in sample_data.columns if col != 'timestamp']
            
            # ä¸€æ¬¡æ€§èµ‹å€¼æ‰€æœ‰æŒ‡æ ‡æ•°æ®
            num_time_steps = min(len(sample_data), 20)
            metric_data[instance_idx, :num_time_steps, :] = sample_data[metric_names].values[:num_time_steps]
        
        except Exception:
            continue
    
    # å°†NaNå€¼æ›¿æ¢ä¸º0
    metric_data = np.nan_to_num(metric_data, nan=0.0)
    
    # å½’ä¸€åŒ–
    if NORMALIZATION_STATS['metric'] is not None:
        stats = NORMALIZATION_STATS['metric']
        metric_data = (metric_data - stats['mean']) / stats['std']
    
    return metric_data

def _process_log_for_sample(st_time, ed_time) -> np.ndarray:
    """
    å¤„ç†å•ä¸ªæ ·æœ¬çš„logæ•°æ®ï¼ˆä½¿ç”¨é¢„åŠ è½½çš„ç¼“å­˜ï¼‰
    
    Args:
        st_time: æ•…éšœå¼€å§‹æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        ed_time: æ•…éšœç»“æŸæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
    
    Returns:
        numpy array, shape [10, 40] - 10ä¸ªinstanceï¼Œæ¯ä¸ª40ç»´templateç»Ÿè®¡
    """
    # ä½¿ç”¨å…¨å±€å®šä¹‰çš„æœåŠ¡é¡ºåº
    num_instances = len(SERVICES)
    num_templates = 40  # å›ºå®š40ä¸ªtemplate
    
    # åˆå§‹åŒ–ç»“æœæ•°ç»„ [num_instances, num_templates]
    log_data = np.zeros((num_instances, num_templates))
    
    # æŒ‰ç…§å›ºå®šé¡ºåºéå†æ¯ä¸ªinstance
    for instance_idx, instance_name in enumerate(SERVICES):
        # ä»ç¼“å­˜ä¸­è¯»å–æ•°æ®
        if instance_name not in LOG_DATA_CACHE:
            continue
        
        try:
            df = LOG_DATA_CACHE[instance_name]
            
            # ç­›é€‰æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
            mask = (df['timestamp_ts'] >= st_time) & (df['timestamp_ts'] <= ed_time)
            sample_data = df[mask]
            
            if len(sample_data) > 0:
                # ç»Ÿè®¡æ¯ä¸ªtemplate_idå‡ºç°çš„æ¬¡æ•°
                template_counts = sample_data['template_id'].value_counts()
                
                # å°†ç»Ÿè®¡ç»“æœå¡«å…¥å¯¹åº”ä½ç½®ï¼ˆtemplate_idä»1å¼€å§‹ï¼Œæ•°ç»„ç´¢å¼•ä»0å¼€å§‹ï¼‰
                for template_id, count in template_counts.items():
                    if 1 <= template_id <= num_templates:
                        log_data[instance_idx, template_id - 1] = count
        
        except Exception:
            continue
    
    # å½’ä¸€åŒ–
    if NORMALIZATION_STATS['log'] is not None:
        stats = NORMALIZATION_STATS['log']
        log_data = (log_data - stats['mean']) / stats['std']
    
    return log_data


def _process_trace_for_sample(st_time, ed_time) -> np.ndarray:
    """
    å¤„ç†å•ä¸ªæ ·æœ¬çš„traceæ•°æ®ï¼ˆä½¿ç”¨é¢„åŠ è½½çš„ç¼“å­˜ï¼‰
    
    Args:
        st_time: æ•…éšœå¼€å§‹æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
        ed_time: æ•…éšœç»“æŸæ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰
    
    Returns:
        np.ndarray: shape [10, 20, 1] - 10ä¸ªinstanceï¼Œ20ä¸ªæ—¶é—´æ®µï¼Œæ¯ä¸ªæ—¶é—´æ®µçš„å¹³å‡duration
                    å¦‚æœæŸä¸ªæ—¶é—´æ®µæ²¡æœ‰æ•°æ®ï¼Œåˆ™ä¸º0.0
    """
    # ä½¿ç”¨å…¨å±€å®šä¹‰çš„æœåŠ¡é¡ºåº
    num_instances = len(SERVICES)
    num_time_segments = 20  # 20ä¸ªæ—¶é—´æ®µ
    segment_duration = 30 * 1000  # æ¯ä¸ªæ—¶é—´æ®µ30ç§’ï¼ˆæ¯«ç§’ï¼‰
    
    # åˆå§‹åŒ–ç»“æœæ•°ç»„ [num_instances, num_time_segments, 1]ï¼Œé»˜è®¤å€¼ä¸º0.0
    trace_data = np.zeros((num_instances, num_time_segments, 1))
    
    # æŒ‰ç…§å›ºå®šé¡ºåºéå†æ¯ä¸ªinstance
    for instance_idx, instance_name in enumerate(SERVICES):
        # ä»ç¼“å­˜ä¸­è¯»å–æ•°æ®
        if instance_name not in TRACE_DATA_CACHE:
            continue
        
        try:
            df = TRACE_DATA_CACHE[instance_name]
            
            # ç­›é€‰æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
            mask = (df['start_time_ts'] >= st_time) & (df['start_time_ts'] <= ed_time)
            sample_data = df[mask]
            
            if len(sample_data) > 0:
                # å‘é‡åŒ–è®¡ç®—ï¼šæ‰¹é‡è®¡ç®—æ‰€æœ‰traceçš„æ—¶é—´æ®µç´¢å¼•
                timestamps = sample_data['start_time_ts'].values
                durations = sample_data['duration'].values
                
                # æ‰¹é‡è®¡ç®—æ—¶é—´åç§»å’Œæ®µç´¢å¼•
                time_offsets = timestamps - st_time
                segment_indices = (time_offsets // segment_duration).astype(int)
                
                # ç­›é€‰æœ‰æ•ˆçš„æ®µç´¢å¼•
                valid_mask = (segment_indices >= 0) & (segment_indices < num_time_segments)
                valid_segments = segment_indices[valid_mask]
                valid_durations = durations[valid_mask]
                
                # æŒ‰æ®µç´¢å¼•åˆ†ç»„è®¡ç®—å¹³å‡å€¼
                for seg_idx in range(num_time_segments):
                    seg_mask = valid_segments == seg_idx
                    if seg_mask.any():
                        trace_data[instance_idx, seg_idx, 0] = valid_durations[seg_mask].mean()
        
        except Exception:
            continue
    
    # å½’ä¸€åŒ–ï¼ˆæŒ‰instanceï¼‰
    if NORMALIZATION_STATS['trace'] is not None:
        for i in range(num_instances):
            stats = NORMALIZATION_STATS['trace'][i]
            trace_data[i, :, 0] = (trace_data[i, :, 0] - stats['mean']) / stats['std']
    
    return trace_data


def _process_single_sample(row) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªæ•…éšœæ ·æœ¬
    """
    sample_id = row['index']
    fault_service = row['instance']
    fault_type = row['anomaly_type']
    st_time = pd.to_datetime(row['st_time']).timestamp() * 1000
    ed_time = st_time + 600 * 1000  # å¼€å§‹æ—¶é—´ + 600ç§’
    data_type = row['data_type']

    processed_sample = {
        'sample_id': sample_id,
        'fault_service': fault_service,
        'fault_type': fault_type,
        'st_time': st_time,
        'ed_time': ed_time,
        'data_type': data_type,
    }

    # å¤„ç†å„æ¨¡æ€æ•°æ®
    processed_sample['metric_data'] = _process_metric_for_sample(st_time, ed_time)
    processed_sample['log_data'] = _process_log_for_sample(st_time, ed_time)
    processed_sample['trace_data'] = _process_trace_for_sample(st_time, ed_time)
    
    return processed_sample


def process_all_sample(label_df) -> Dict[int, Dict[str, Any]]:
    """
    å¤„ç†æ‰€æœ‰æ•…éšœæ ·æœ¬
    """
    processed_data = {}
    
    print(f"\nå¼€å§‹å¤„ç† {len(label_df)} ä¸ªæ•…éšœæ ·æœ¬...")
    
    for idx, row in tqdm(label_df.iterrows(), total=len(label_df), desc="Processing samples"):
        sample_id = row['index']
        try:
            processed_sample = _process_single_sample(row)
            processed_data[sample_id] = processed_sample
        except Exception as e:
            print(f"\nâŒ Failed to process sample {sample_id}: {e}")
            continue
    
    print(f"\nâœ… å®Œæˆï¼æˆåŠŸå¤„ç† {len(processed_data)}/{len(label_df)} ä¸ªæ ·æœ¬")
    return processed_data


if __name__ == "__main__":
    import pickle
    
    project_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    label_file = os.path.join(project_dir, "extractor", "MicroSS", "label.csv")
    label_df = pd.read_csv(label_file)
    
    # 1. é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
    preload_all_data()
    
    # 2. è®¡ç®—æˆ–åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯
    stats_file = os.path.join(project_dir, "preprocess", "processed_data", "norm_stats.pkl")
    
    if os.path.exists(stats_file):
        print(f"\nğŸ“‚ åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡: {stats_file}")
        with open(stats_file, 'rb') as f:
            stats = pickle.load(f)
    else:
        print("\nğŸ”„ è®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡...")
        stats = compute_normalization_stats(label_df)
        with open(stats_file, 'wb') as f:
            pickle.dump(stats, f)
        print(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_file}")
    
    # è®¾ç½®å…¨å±€ç»Ÿè®¡ä¿¡æ¯
    NORMALIZATION_STATS['metric'] = stats['metric']
    NORMALIZATION_STATS['log'] = stats['log']
    NORMALIZATION_STATS['trace'] = stats['trace']
    
    # 3. å¤„ç†æ‰€æœ‰æ ·æœ¬
    processed_data = process_all_sample(label_df)
    
    # 4. ä¿å­˜å¤„ç†åçš„æ•°æ®
    output_file = os.path.join(project_dir, "preprocess", "processed_data", "dataset.pkl")
    with open(output_file, 'wb') as f:
        pickle.dump(processed_data, f)
    print(f"\nğŸ’¾ æ•°æ®é›†å·²ä¿å­˜: {output_file}")
    print(f"   - æ ·æœ¬æ•°: {len(processed_data)}")
    print(f"   - å·²å½’ä¸€åŒ–: Metric, Log, Trace")