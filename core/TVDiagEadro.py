import os
import time
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import random
import dgl
import numpy as np

from core.ita import cal_task_affinity
from core.loss.AutomaticWeightedLoss import AutomaticWeightedLoss
from core.model.MainModelEadro import MainModelEadro
from helper.eval import *
from helper.early_stop import EarlyStopping
from helper.Result import Result
from config.exp_config import Config


class TVDiagEadro(object):
    """é›†æˆEadroç¼–ç å™¨çš„TVDiagè®­ç»ƒå’Œè¯„ä¼°æ¡†æ¶"""

    def __init__(self, config: Config, logger, log_dir: str):
        self.config = config
        self.logger = logger
        os.makedirs(log_dir, exist_ok=True)

        use_gpu = torch.cuda.is_available()
        if use_gpu:
            logger.info("Currently using GPU {}".format(config.gpu_device))
            os.environ['CUDA_VISIBLE_DEVICES'] = config.gpu_device
            self.device = 'cuda'
        else:
            logger.info("Currently using CPU (GPU is highly recommended)")
            self.device = 'cpu'

        self.result = Result()
        self.writer = SummaryWriter(log_dir)
        self.printParams()

    def printParams(self):
        self.config.print_configs(self.logger)

    def train(self, train_data, val_data, aug_data):
        model = MainModelEadro(self.config).to(self.device)
        opt = torch.optim.Adam(model.parameters(), lr=self.config.lr, weight_decay=self.config.weight_decay)
        
        awl = AutomaticWeightedLoss(2)  # åªæœ‰2ä¸ªæŸå¤±ï¼šl_rcl å’Œ l_fti

        self.logger.info(model)
        self.logger.info(f"Start training for {self.config.epochs} epochs.")
        
        train_times = []
        Z_r2fs, Z_f2rs = [], []
        
        earlyStop = EarlyStopping(patience=self.config.patience)
        best_model_path = os.path.join(self.writer.log_dir, 'TVDiagEadro_best.pt')
        
        for epoch in range(self.config.epochs):
            n_iter = 0
            start_time = time.time()
            model.train()
            epoch_loss, epoch_rcl_l, epoch_fti_l = 0, 0, 0
            rcl_results = {"HR@1": [], "HR@2": [], "HR@3": [], "HR@4": [],"HR@5": [], "MRR@3": []}
            fti_results = {'pre':[], 'rec':[], 'f1':[]}

            train_dl = DataLoader(train_data, batch_size=self.config.batch_size, shuffle=True, collate_fn=self.collate)
            for batch_graphs, batch_labels in train_dl:
                batch_graphs = batch_graphs
                instance_labels = batch_labels[:, 0]
                type_labels = batch_labels[:, 1]

                if self.config.aug_times > 0:
                    raw_graphs = dgl.unbatch(batch_graphs)
                    aug_graphs, aug_labels = map(list, zip(*random.sample(aug_data, len(raw_graphs))))
                    batch_graphs = dgl.batch(raw_graphs + aug_graphs)
                    instance_labels = torch.hstack((instance_labels, torch.tensor(aug_labels)[:,0].flatten()))
                    type_labels = torch.hstack((type_labels, torch.tensor(aug_labels)[:,1].flatten()))

                batch_graphs = batch_graphs.to(self.device)
                instance_labels = instance_labels.to(self.device)
                type_labels = type_labels.to(self.device)

                opt.zero_grad()
                
                # å¤šåœºæ™¯æ•°æ®æ‰©å±•æˆ–æ™®é€šè®­ç»ƒ
                if (getattr(self.config, 'use_modality_dropout', False) and 
                    getattr(self.config, 'modality_dropout_mode', 'random') == 'multi_scenario' and
                    getattr(self.config, 'use_cross_modal_attention', False)):
                    # å¤šåœºæ™¯è®­ç»ƒï¼šå°†batchæ‰©å±•ä¸º4ç§æ¨¡æ€é…ç½®çš„æ··åˆbatch
                    expanded_graphs, expanded_type_labels = self._expand_batch_multi_scenario(batch_graphs, type_labels)
                    fs, es, root_logit, type_logit = model(expanded_graphs)
                    l_rcl = self.cal_rcl_loss(root_logit, expanded_graphs)
                    l_fti = F.cross_entropy(type_logit, expanded_type_labels)
                else:
                    # æ™®é€šè®­ç»ƒ
                    fs, es, root_logit, type_logit = model(batch_graphs)
                    l_rcl = self.cal_rcl_loss(root_logit, batch_graphs)
                    l_fti = F.cross_entropy(type_logit, type_labels)
                
                if self.config.dynamic_weight:
                    total_loss = awl(l_rcl, l_fti)
                else:
                    total_loss = l_rcl + l_fti

                total_loss.backward()
                opt.step()
                
                epoch_loss += total_loss.detach().item()
                epoch_rcl_l += l_rcl.detach().item()
                epoch_fti_l += l_fti.detach().item()

                # åœ¨å¤šåœºæ™¯æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨åŸå§‹batchè¿›è¡Œè¯„ä¼°
                if (getattr(self.config, 'use_modality_dropout', False) and 
                    getattr(self.config, 'modality_dropout_mode', 'random') == 'multi_scenario' and
                    getattr(self.config, 'use_cross_modal_attention', False)):
                    # å¤šåœºæ™¯æ¨¡å¼ï¼šä½¿ç”¨åŸå§‹batchï¼ˆå®Œæ•´æ¨¡æ€ï¼‰è¿›è¡Œè¯„ä¼°
                    _, _, eval_root_logit, eval_type_logit = model(batch_graphs)
                    rcl_res = RCA_eval(eval_root_logit, batch_graphs.batch_num_nodes(), batch_graphs.ndata['root'])
                    fti_res = FTI_eval(eval_type_logit, type_labels)
                else:
                    # æ™®é€šæ¨¡å¼ï¼šä½¿ç”¨è®­ç»ƒçš„é¢„æµ‹ç»“æœ
                    rcl_res = RCA_eval(root_logit, batch_graphs.batch_num_nodes(), batch_graphs.ndata['root'])
                    fti_res = FTI_eval(type_logit, type_labels)
                
                [rcl_results[key].append(value) for key, value in rcl_res.items()]
                [fti_results[key].append(value) for key, value in fti_res.items()]
                n_iter += 1
                
            mean_epoch_loss = epoch_loss / n_iter
            mean_rcl_loss = epoch_rcl_l / n_iter
            mean_fti_loss = epoch_fti_l / n_iter
            end_time = time.time()
            time_per_epoch = (end_time - start_time)
            train_times.append(time_per_epoch)
            
            self.logger.info("Epoch {} done. Loss: {:.3f}, Time per epoch: {:.3f}[s]"
                         .format(epoch, mean_epoch_loss, time_per_epoch))

            for k, v in rcl_results.items():
                rcl_results[k] = np.mean(v)
            for k, v in fti_results.items():
                fti_results[k] = np.mean(v)
                
            self.writer.add_scalar('loss/train_total_loss', mean_epoch_loss, global_step=epoch)
            self.writer.add_scalar('train/HR@3', rcl_results['HR@3'], global_step=epoch)
            self.writer.add_scalar('train/f1-score', fti_results['f1'], global_step=epoch)

            # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
            val_loss, val_rcl, val_fti = self._validate(model, val_data)
            
            self.writer.add_scalar('loss/val_total_loss', val_loss, global_step=epoch)
            self.writer.add_scalar('val/HR@3', val_rcl['HR@3'], global_step=epoch)
            self.writer.add_scalar('val/f1-score', val_fti['f1'], global_step=epoch)
            
            self.logger.info(f"Val Loss: {val_loss:.3f}, Val HR@3: {val_rcl['HR@3']:.3%}, Val F1: {val_fti['f1']:.3%}")

            # æ—©åœåˆ¤æ–­ï¼ˆåŸºäºéªŒè¯é›†lossï¼‰
            stop, is_best = earlyStop.should_stop(val_loss, epoch)
            
            if is_best:
                # ä¿å­˜æœ€ä¼˜æ¨¡å‹
                state = {
                    'epoch': epoch,
                    'model': model.state_dict(),
                    'opt': opt.state_dict(),
                    'val_loss': val_loss,
                }
                torch.save(state, best_model_path)
                self.logger.info(f"âœ“ Best model saved at epoch {epoch} with val_loss: {val_loss:.3f}")
            
            if stop:
                self.logger.info(f"Early stop at epoch {epoch} due to no improvement on validation set.")
                break

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆæœ€åä¸€è½®ï¼‰
        state = {
            'epoch': epoch,
            'model': model.state_dict(),
            'opt': opt.state_dict(),
        }
        torch.save(state, os.path.join(self.writer.log_dir, 'TVDiagEadro_last.pt'))
        self.result.set_train_efficiency(train_times)
        self.logger.info("Training has finished.")
        self.logger.info(f"Best model saved at: {best_model_path}")

    def _validate(self, model, val_data):
        """
        åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹
        
        Returns:
            tuple: (val_loss, rcl_results, fti_results)
        """
        model.eval()
        val_loss, val_rcl_l, val_fti_l = 0, 0, 0
        rcl_results = {"HR@1": [], "HR@2": [], "HR@3": [], "HR@4": [], "HR@5": [], "MRR@3": []}
        fti_results = {'pre': [], 'rec': [], 'f1': []}
        n_iter = 0
        
        val_dl = DataLoader(val_data, batch_size=self.config.batch_size, shuffle=False, collate_fn=self.collate)
        
        with torch.no_grad():
            for batch_graphs, batch_labels in val_dl:
                batch_graphs = batch_graphs.to(self.device)
                instance_labels = batch_labels[:, 0].to(self.device)
                type_labels = batch_labels[:, 1].to(self.device)
                
                fs, es, root_logit, type_logit = model(batch_graphs)
                
                # åªè®¡ç®—ä¸»ä»»åŠ¡æŸå¤±
                l_rcl = self.cal_rcl_loss(root_logit, batch_graphs)
                l_fti = F.cross_entropy(type_logit, type_labels)
                
                total_loss = l_rcl + l_fti
                
                val_loss += total_loss.detach().item()
                val_rcl_l += l_rcl.detach().item()
                val_fti_l += l_fti.detach().item()
                
                rcl_res = RCA_eval(root_logit, batch_graphs.batch_num_nodes(), batch_graphs.ndata['root'])
                fti_res = FTI_eval(type_logit, type_labels)
                [rcl_results[key].append(value) for key, value in rcl_res.items()]
                [fti_results[key].append(value) for key, value in fti_res.items()]
                n_iter += 1
        
        mean_val_loss = val_loss / n_iter
        for k, v in rcl_results.items():
            rcl_results[k] = np.mean(v)
        for k, v in fti_results.items():
            fti_results[k] = np.mean(v)
        
        model.train()  # æ¢å¤è®­ç»ƒæ¨¡å¼
        return mean_val_loss, rcl_results, fti_results

    def evaluate(self, test_data, model=None):
        if model is None:
            # åŠ è½½æœ€ä¼˜æ¨¡å‹æƒé‡
            best_model_path = os.path.join(self.writer.log_dir, 'TVDiagEadro_best.pt')
            if os.path.exists(best_model_path):
                self.logger.info(f"Loading best model from {best_model_path}")
                checkpoint = torch.load(best_model_path)
            else:
                # å¦‚æœæ²¡æœ‰æœ€ä¼˜æ¨¡å‹ï¼ŒåŠ è½½æœ€åçš„æ¨¡å‹
                self.logger.info("Best model not found, loading last model")
                checkpoint = torch.load(os.path.join(self.writer.log_dir, 'TVDiagEadro_last.pt'))
            
            model = MainModelEadro(self.config).to(self.device)
            model.load_state_dict(checkpoint['model'])
            self.logger.info(f"Model loaded from epoch {checkpoint['epoch']}")
       
        model.eval()
        root_logits, type_logits = [], []
        roots, types = [], []
        inference_times = []
        num_node_list = []
        
        for data in test_data:
            graph = data[0].to(self.device)
            failure_type = data[1][1]
            roots.append(graph.ndata['root'])
            types.append(failure_type)
            num_node_list.append(graph.num_nodes())
        
            start_time = time.time()
            with torch.no_grad():
                _, _, root_logit, type_logit = model(graph)
                root_logits.append(root_logit.flatten())
                type_logits.append(type_logit.flatten())
            end_time = time.time()
            inference_times.append(end_time - start_time)
            
        root_logits = torch.hstack(root_logits).cpu()
        type_logits = torch.vstack(type_logits).cpu()
        roots = torch.hstack(roots)
        types = torch.tensor(types)

        rcl_res = RCA_eval(root_logits, num_node_list, roots)
        fti_res = FTI_eval(type_logits, types)
        self.result.set_performance(rcl_res, fti_res)
        self.result.set_inference_efficiency(inference_times)

        avg_3 = np.mean([rcl_res['HR@1'], rcl_res['HR@2'], rcl_res['HR@3']])

        self.logger.info("[Root localization] HR@1: {:.3%}, HR@2: {:.3%}, HR@3: {:.3%}, HR@4: {:.3%}, HR@5: {:.3%}, avg@3: {:.3f}, MRR@3: {:.3f}"\
            .format(rcl_res['HR@1'], rcl_res['HR@2'], rcl_res['HR@3'], rcl_res['HR@4'], rcl_res['HR@5'] , avg_3, rcl_res['MRR@3']))
        self.logger.info("[Failure type classification] precision: {:.3%}, recall: {:.3%}, f1-score: {:.3%}"\
            .format(fti_res['pre'], fti_res['rec'], fti_res['f1']))
        self.logger.info(f"The average test time is {np.mean(inference_times)}[s]")

        return self.result

    def cal_rcl_loss(self, root_logit, batch_graphs):        
        num_nodes_list = batch_graphs.batch_num_nodes()
        total_loss = None
        
        start_idx = 0
        for idx, num_nodes in enumerate(num_nodes_list):
            end_idx = start_idx + num_nodes
            node_logits = root_logit[start_idx : end_idx].reshape(1, -1)
            root = batch_graphs.ndata["root"][start_idx : end_idx].tolist().index(1)
            loss = F.cross_entropy(node_logits, torch.LongTensor([root]).view(1).to(self.device))
            if total_loss is None:
                total_loss = loss
            else:
                total_loss += loss
            start_idx += num_nodes
            
        l_rcl = total_loss / len(num_nodes_list)
        return l_rcl

    def collate(self, samples):
        graphs, labels = map(list, zip(*samples))
        batched_graph = dgl.batch(graphs)
        batched_labels = torch.tensor(labels)
        return batched_graph, batched_labels

    def _expand_batch_multi_scenario(self, batch_graphs, type_labels):
        """
        å¤šåœºæ™¯batchæ‰©å±•ï¼šå°†åŸå§‹batchæ‰©å±•ä¸º1.5xå¤§å°çš„æ··åˆbatch
        
        æ¯”ä¾‹åˆ†é…ï¼š
        - å®Œæ•´æ¨¡æ€ï¼šNä¸ªæ ·æœ¬ï¼ˆä¿æŒåŸæœ‰æ•°é‡ï¼‰
        - ç¼ºå¤±æ¨¡æ€ï¼šN/2ä¸ªæ ·æœ¬ï¼ˆæ€»å…±ï¼Œå¹³å‡åˆ†é…ç»™3ç§ç¼ºå¤±æƒ…å†µï¼‰
        - æ€»æ ·æœ¬æ•°ï¼š1.5N
        
        Args:
            batch_graphs: åŸå§‹batchå›¾æ•°æ®
            type_labels: åŸå§‹ç±»å‹æ ‡ç­¾
            
        Returns:
            expanded_graphs: æ‰©å±•åçš„å›¾æ•°æ®ï¼ˆ1.5xå¤§å°ï¼‰
            expanded_type_labels: æ‰©å±•åçš„ç±»å‹æ ‡ç­¾ï¼ˆ1.5xå¤§å°ï¼‰
        """
        
        # è§£batchè·å–å•ä¸ªå›¾
        graph_list = dgl.unbatch(batch_graphs)
        original_batch_size = len(graph_list)
        
        # å¦‚æœå¯ç”¨äº†æ•°æ®å¢å¼ºï¼ŒbatchåŒ…å«åŸå§‹æ•°æ®+å¢å¼ºæ•°æ®ï¼Œéœ€è¦åªä»åŸå§‹æ•°æ®ä¸­é€‰æ‹©
        if self.config.aug_times > 0:
            # batchç»“æ„ï¼šå‰åŠéƒ¨åˆ†æ˜¯åŸå§‹æ•°æ®ï¼ŒååŠéƒ¨åˆ†æ˜¯å¢å¼ºæ•°æ®
            original_data_size = original_batch_size // 2
            original_graphs = graph_list[:original_data_size]  # åªå–åŸå§‹æ•°æ®
            original_labels = type_labels[:original_data_size]
            print(f"   ğŸ” æ£€æµ‹åˆ°æ•°æ®å¢å¼º: batch={original_batch_size}, ä»…ä½¿ç”¨åŸå§‹æ•°æ®={original_data_size}")
        else:
            # æ²¡æœ‰æ•°æ®å¢å¼ºï¼Œæ•´ä¸ªbatchéƒ½æ˜¯åŸå§‹æ•°æ®
            original_graphs = graph_list
            original_labels = type_labels
            original_data_size = original_batch_size
        
        # è®¡ç®—å„ç§é…ç½®çš„æ ·æœ¬æ•°é‡ï¼ˆåŸºäºåŸå§‹æ•°æ®å¤§å°ï¼‰
        full_modality_count = original_data_size  # å®Œæ•´æ¨¡æ€æ ·æœ¬æ•°
        missing_ratio = getattr(self.config, 'missing_modality_ratio', 0.5)  # ç¼ºå¤±æ¨¡æ€æ¯”ä¾‹
        missing_modality_total = int(original_data_size * missing_ratio)  # ç¼ºå¤±æ¨¡æ€æ€»æ•°
        missing_per_type = missing_modality_total // 3  # æ¯ç§ç¼ºå¤±ç±»å‹çš„æ ·æœ¬æ•°
        
        # å¤„ç†ä¸èƒ½æ•´é™¤çš„æƒ…å†µï¼Œä¼˜å…ˆåˆ†é…ç»™ç¼ºmetric
        remaining = missing_modality_total - missing_per_type * 3
        missing_metric_count = missing_per_type + remaining
        missing_log_count = missing_per_type
        missing_trace_count = missing_per_type
        
        print(f"   ğŸ“Š ç¼ºå¤±æ¨¡æ€æ¯”ä¾‹: {missing_ratio:.1f} (æ€»ç¼ºå¤±={missing_modality_total})")
        print(f"   ğŸ“Š æ ·æœ¬åˆ†é…: å®Œæ•´={full_modality_count}, ç¼ºmetric={missing_metric_count}, ç¼ºlog={missing_log_count}, ç¼ºtrace={missing_trace_count}")
        
        # å®šä¹‰æ¨¡æ€é…ç½®å’Œå¯¹åº”æ•°é‡
        config_specs = [
            ({'metric': True, 'log': True, 'trace': True}, full_modality_count),    # å®Œæ•´
            ({'metric': False, 'log': True, 'trace': True}, missing_metric_count),  # ç¼ºmetric
            ({'metric': True, 'log': False, 'trace': True}, missing_log_count),     # ç¼ºlog  
            ({'metric': True, 'log': True, 'trace': False}, missing_trace_count)    # ç¼ºtrace
        ]
        
        expanded_graphs = []
        expanded_labels = []
        modality_masks = []  # æ”¶é›†æ‰€æœ‰çš„æ¨¡æ€æ©ç 
        
        # ä¸ºæ¯ç§é…ç½®ç”Ÿæˆå¯¹åº”æ•°é‡çš„æ ·æœ¬
        for config_idx, (config, count) in enumerate(config_specs):
            is_full_modality = (config_idx == 0)  # ç¬¬ä¸€ä¸ªé…ç½®æ˜¯å®Œæ•´æ¨¡æ€
            
            for i in range(count):
                if is_full_modality:
                    # å®Œæ•´æ¨¡æ€ï¼šé¡ºåºä½¿ç”¨æ‰€æœ‰åŸå§‹æ•°æ®ï¼ˆç¡®ä¿è¦†ç›–å®Œæ•´ï¼‰
                    graph_idx = i % original_data_size
                else:
                    # ç¼ºå¤±æ¨¡æ€ï¼šéšæœºé€‰æ‹©ï¼ˆå¢åŠ è®­ç»ƒå¤šæ ·æ€§ï¼‰
                    graph_idx = random.randint(0, original_data_size - 1)
                
                graph = original_graphs[graph_idx]
                
                # å¤åˆ¶å›¾
                new_graph = graph.clone()
                expanded_graphs.append(new_graph)
                expanded_labels.append(original_labels[graph_idx].item())
                
                # æ”¶é›†æ¨¡æ€æ©ç 
                mask = torch.tensor([
                    config['metric'], config['log'], config['trace']
                ], dtype=torch.bool).to(graph.device)
                modality_masks.append(mask)
        
        # é‡æ–°batchåŒ–
        expanded_batch_graphs = dgl.batch(expanded_graphs)
        expanded_type_labels = torch.tensor(expanded_labels, dtype=original_labels.dtype).to(original_labels.device)
        
        # å°†æ¨¡æ€æ©ç å­˜å‚¨åœ¨batchå›¾çš„å±æ€§ä¸­
        expanded_batch_graphs.modality_masks = torch.stack(modality_masks)  # [batch_size, 3]
        
        total_samples = len(expanded_labels)
        print(f"   âœ… Batchæ‰©å±•: åŸå§‹æ•°æ®{original_data_size} â†’ {total_samples} (æ‰©å±•å€æ•°: {total_samples/original_data_size:.1f}x)")
        
        return expanded_batch_graphs, expanded_type_labels


